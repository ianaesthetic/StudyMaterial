\documentclass[10pt, a4paper]{article}
    \author{Ianaesthetic}
    \title{Real-Time Rendering Note}
    \usepackage{indentfirst, amsmath, color, amssymb}
\begin{document}
\maketitle
\newpage
\section{Introduction\\}

\section{The Graphics Rendering Pipeline}
    \emph{Pipeline}: to generate, or render, a two-dimensional images in a three dimensional environment.
        \subsection{Architecture}
        A pipeline consists of several \emph{stages}. Rendering speed depends on the \emph{bottleneck (the slowest pipeline stage)}.
        \\

        \emph{Conceptual Stages}: Including \emph{Application Stage}, \emph{Geometry Stage}, \emph{Rasterizer Stage}.\\
        \indent\emph{Functional Stages}: Has certain tasks to perform without specifying the way that task is executed in the pipeline.\\
        \indent\emph{Pipeline Stages}: The actual implementations of the functional stages. It may combine or divide functional stages in order to get better performance.\\
        
        The rendering speed is expressed in \emph{frames per seconds (fps)} or \emph{Hertz (Hz)}. Hertz is for hardware. The output fps is determined by both pipeline and display. \\

        \subsection{The Application Stage}
        Application Stage is executed on the CPU and can be fully controlled by developers. At the end of the application stage, \emph{rendering primitive} including points, edges and other geometric elements are generated and passed onto geometry stage. \\
        \indent Application stage hasn't got any substage, and is charge of collision detection, input interaction and acceleration algorithm.\\

        \subsection{The Geometry stage} 
        Geometry stage is responsible for per polygon and vertex operations, including several pipeline stages.\\
        \begin{center}
        \emph{\scriptsize  Model \& View Transform} $\to$ \emph{\scriptsize Vertex Shading} $\to$ \emph{\scriptsize Projection} $\to$ \emph{\scriptsize Clipping} $\to$ \emph{\scriptsize Screen Mapping}
        \end{center}
            
            \subsubsection{Model and View Transform}
            every model has its own \emph{model space}, and is associated with \emph{model transforms} determining its orientation and positions. Thus, a single model can have different copies called \emph{instances}.\\
            \begin{center}
            \noindent \emph{\footnotesize Model Space} $\xrightarrow{\tiny \emph{Model Transform}}$ \emph{\footnotesize World Space} $\xrightarrow{\tiny \emph{View Transform}}$ \emph{\footnotesize Camera Space}
            \end{center}
            \newpage
            World Space is unique. In Camera Space, the camera locates at the origin and faces the negative z direction commonly.
            
            \subsubsection{Vertex Shading}
                The appearance of objects are related to materials and light. \emph{Shading} determines the effect of light on specific material using \emph{shading equation}. Those computations are performed in geometry stage per vertex or in rasterizer stage per pixel.
            
            \subsubsection{Projection}
                \emph{Projection} transforms the view volume to a unit cube(\emph{canonical view volume}). Projection methods involve \emph{orthographic} and \emph{perspective}.After Projection, the models are said to be in \emph{normalized device coordinate}.
            
            \subsubsection{Clipping} 
                Primitives that are partially inside the canonical view volume require \emph{clipping} as only the part inside the view volume will be rendered. The previous stages are performed by programmable processing unit, while clipping is performed by fixed-operation hardware. 
            
            \subsubsection{Screen Mapping} 
                \emph{Screen Mapping} maps normalized device coordinate to screen coordinate. DirectX 9 and its predecessors define the center of first pixel as $(0, 0)$. The successors of DirectX 10 and OpenGL define the center as $(0, 0)$, resulting a convenient conversions:\\
                \begin{center}
                    \large{\emph{d} = \textbf{\emph{floor}}(\emph{c})}\\
                    \large{\emph{c} = \emph{d} + 0.5}
                \end{center}

                \indent Different API puts the first pixel in different places. 
        
        \subsection{The Rasterizer Stage}
            Given the transformed and projected vertices and associated shading data, the Rasterizer Stage will set the color of every pixels. This process is called \emph{rasterization} or \emph{scan conversion}.
            \begin{center}
                \emph{Triangle Setup} $\to$ \emph{Triangle Traversal} $\to$ \emph{Pixel Shading} $\to$ \emph{Merging}
            \end{center}

            \subsubsection{Triangle Setup}
            In this stage the differentials and other data for the triangle's surface are computed. This stage is performed by fixed-operation hardware dedicated to this task.
            
            \subsubsection{Triangle Traversal} 
            Finding which samples or pixels are inside a triangle is often called \emph{Triangle Traversal} or \emph{Scan Conversion}.
            
            \subsubsection{Pixel Shading} 
            Any per-pixel shading is here and resulting more color data passed to next stage. This stage is executed by programmable process unit. A lot of important technics such as \emph{texturing} is employed here. 

            \subsubsection{Merging} 
            The information for every pixel is stored in \emph{color buffer}. \emph{Merging} is responsible for combine the fragment color with the color in the buffer and visibility. This stage is not programmable but highly configurable. Visibility is done by \emph{Z-Buffer} or \emph{Depth-Buffer} by update the smallest z value. However, when rendering transparent primitives, all opaque primitives must be rendered first and then rendered other in a \textbf{back-to-front} order. \\
            \indent The \emph{alpha channel} is associated with color buffer and stores a related opacity value. The \emph{stencil buffer} is an off-screen buffer used to record the locations of rendered primitives. It derives from primitives and can be used to control rendering into color buffer and z-buffer. All functions above are called \emph{raster operation} or \emph{blend operation}.\\
            \indent There are also \emph{frame buffer}(consists of all the buffer), \emph{accumulation buffer}(complement to frame buffer), \emph{double buffer}(for display).
    
        \subsection{Through the Pipeline}
        \newpage
    


    \section{The Graphics Processing Unit}
        \emph{Graphics processing unit}(\emph{GPU}) is different from previous rasterization-only chip, evolve from configuration implementations of complex fixed operation to highly programmable blank-slates. Programmable \emph{shader} is the primary means.Vertex Shader and Pixel Shader allow operations per Vertex and pixel. Geometry shader allows create and destroy primitives on the fly. 
        
        \subsection{GPU Pipeline Overview}
            \noindent Vertex Shader \hfill\emph{fully programmable}\\
            Geometry Shader \hfill\emph{fully programmable}\\
            Clipping \hfill\emph{configurable}\\
            Screen Mapping \hfill\emph{completely fixed}\\
            Triangle Setup \hfill\emph{completely fixed}\\
            Triangle Traversal \hfill\emph{completely fixed}\\
            Pixel Shader \hfill\emph{fully programmable}\\
            Merger \hfill\emph{configurable}\\
            
            The Geometry shader is optional.

        \subsection{The Programmable Shader Stage}
            Modern shader stages share a \emph{common shader core}.The common shader core is the API and unified shaders is a GPU features. Shaders are programmed using C-like \emph{shading language} which will be compiled to \emph{intermediate language}(\emph{IL}), and IL will be converted to machine language through drivers. IL can be seen as a virtual machine, with 4-way \emph{single-instruction multiple-data}(SIMD) representing positions, vectors, textures. \emph{Swizzling}, the replication of any vector component, and \emph{masking}, the specific component of vector is used, are supported. \\
            \indent A \emph{draw call} invokes the graphics API to draw a group of primitives, so causing the graphics pipeline to execute. Inputs of shaders involve \emph{uniform} inputs that remain same in the draw call and \emph{varying} inputs. The output of GPU is strictly constrained. Uniform inputs are accessed via \emph{constant register} or \emph{constant buffer}, much more than \emph{varying input register} in which varying inputs lie. There are also \emph{temporary registers} for scratch space.\\
            \indent Common operations are efficient in GPU and it has \emph{intrinsic functions} for complex operations.\\
            \indent The term \emph{flow control} refers to the use of \textbf{branching instructions}(loop is also included) to change the flow of code execution. \emph{Static flow control} depends on uniform inputs while \emph{dynamic flow control} depends on varying inputs.\\
            Shader Programme can be compiled offline and have different output files sent to GPU via drivers as strings according to different situations.
        
        \subsection{The Evolution of Programmable Shading}
        
        \newpage

        \subsection{The Vertex Shader}
             It's worth noticing that some data manipulations happen before this stage called \emph{input assembler}, weaving streams of data to different sets of vertices and primitives. It also performs instancing, which allows an object to be drawn a number of times with different data. \\
             \indent The Vertex Shader first process vertices of triangle shader. Each vertex is processed independently thus they can be applied to parallel.
       
        \subsection{The Geometry Shader} 
            The input to the geometry shader is a single object and its associated vertices. The output can be \textbf{zero} and more primitives. The type of primitives in the input and output can be different. The geometry shader guarantee the output has the same order as the input. This stage is more about programmatically modifying incoming data or making a limited number of copies, rather than massively replicating or amplifying it.

            \subsubsection{Stream Output}
                The idea of \emph{stream output} is to gathered the output from vertex shader and geometry shader in the stream. The rasterization stage can be optionally turned off and data processed in this way can be sent back allowing interaction process. 
        
        \subsection{The Pixel Shader} 
            The rasterizer does not directly affect pixel's color, but generate data to describe how a triangle covers a pixel. Additional inputs are needed for the pixel shader. The pixel shader can only influence the fragment it handles for merging. One exception is dealing with gradient and derivative information, which is the special capability of pixel shader. GPU implements this by processing a group of 2 $\times$ 2 or more. However, as a group of pixels should follow do same operations, no flow control is available here.\\
            \indent\emph{Multiple Rendering Targets}(MRT) is derived as the huge capability of pixel shader, saving resulted color data to different same-dimension even same-bit-depth buffer. For different intermediate images being computed from same data, MRT allows all the rendering being done in one pass. MRT are also related to the abilities to read from these resulting images as textures.

        \subsection{The Merging Stage}
            This stage is where stencil-buffer and Z-buffer operations occur. Other operations such as color blending are involved. Operations are highly configurable. Color blending can be set up to perform a large number of different operations. 
        
        \subsection{Effect}
            As shader program can't be isolated, or some particular effects need rounds of processing, \emph{effect file} is aimed to encapsulate all the relevant information with some arguments to produce certain effects written in \emph{effect language}. 

    \section{Transform} 
        \emph{Transform} is an operation that takes entities such as points, vectors or colors and converts them in some way. A \emph{linear transform} is one that preserve vector addition and scalar multiplication, specifically:$$\textbf{f}(\textbf{x}) + \textbf{f}(\textbf{y}) = \textbf{f}(\textbf{x} + \textbf{y})$$ $$k\textbf{f}(\textbf{x}) = \textbf{f}(k\textbf{x})$$ \indent Scaling and Rotation are both linear transform while translation is not. Combining linear transform and translation, we introduce \emph{affine transform} with 4 $\times$ 4 matrices and \emph{homogeneous notation}. \\
        \indent Vectors are presented as $\textbf{v} = (v_x, v_y, v_z, 0)^T$ and points are $\textbf{v} = (v_x, v_y, v_z, 1)^T$.
        
        \subsection{Basic Transform} 
            
            \subsubsection{Translation}
            \begin{equation*}
                \textbf{T}(\textbf{t}) = \textbf{T}(t_x\ t_y\ t_z) = \begin{pmatrix}
                    1&0&0&t_x\\
                    0&1&0&t_y\\
                    0&0&1&t_z\\
                    0&0&0&1
                \end{pmatrix}
            \end{equation*}
            $$\textbf{T}^{-1}(\textbf{t}) = \textbf{T}(-\textbf{t})$$
            
            \subsubsection{Rotation}
                Rotation, along with Translation, is \emph{rigid-body transform}, which preserves the distances between points transformed, and preserves handedness.\\
                \indent Assume $\textbf{u}$, $\textbf{v}$, $\textbf{w}$ are orthonormal, which means:
                
                \begin{equation*}
                    \textbf{u} \cdot \textbf{u} = \textbf{v} \cdot \textbf{v} = \textbf{w} \cdot \textbf{w} = 1
                \end{equation*}
                    \begin{equation*}
                    \textbf{u} \cdot \textbf{v} = \textbf{v} \cdot \textbf{w} = \textbf{w} \cdot \textbf{u} = 0
                \end{equation*}

                \indent Place three vector Horizontally to form a matrix $\textbf{R}_{uvw}$
                
                \begin{equation*}
                    \textbf{R}_{uvw} = \begin{pmatrix}
                        x_u&y_u&z_u\\
                        x_v&y_v&z_v\\
                        x_w&y_w&z_w
                    \end{pmatrix}
                \end{equation*}

                \indent we can find $\textbf{R}_{uvw}\textbf{u} = \textbf{x}$, $\textbf{R}_{uvw}\textbf{v} = \textbf{y}$, $\textbf{R}_{uvw}\textbf{w} = \textbf{z}$. Take $\textbf{u}$ as example:
                
                \begin{equation*}
                    \textbf{R}_{uvw}\textbf{u}=\begin{pmatrix}
                        x_u&y_u&z_u\\
                        x_v&y_v&z_v\\
                        x_w&y_w&z_w
                    \end{pmatrix}
                    \begin{pmatrix}
                        x_u\\
                        y_u\\
                        z_u
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        1\\
                        0\\
                        0\\
                    \end{pmatrix}
                    =\textbf{x}
                \end{equation*}

                \indent We then consider the inverse of the $\textbf{R}_{uvw}^T$ 
                \begin{equation*}
                    \textbf{R}_{uvw}^T = 
                    \begin{pmatrix}
                        x_u & x_v & x_w\\
                        y_u & y_v & y_w\\
                        z_u & z_v & z_w
                    \end{pmatrix}
                \end{equation*}
                \newpage
                \indent We can find $\textbf{R}_{uvw}^T\textbf{x}=\textbf{u}$, $\textbf{R}_{uvw}^T\textbf{y}=\textbf{v}$, $\textbf{R}_{uvw}^T\textbf{z}=\textbf{w}$. Take $\textbf{x}$ as example:
                \begin{equation*}
                    \textbf{R}_{uvw}^T\textbf{x}=
                    \begin{pmatrix}
                        x_u & x_v & x_w\\
                        y_u & y_v & y_w\\
                        z_y & z_v & z_w
                    \end{pmatrix}
                    \begin{pmatrix}
                        1\\
                        0\\
                        0\\
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        x_u\\
                        y_u\\
                        z_u
                    \end{pmatrix}
                    =\textbf{u}
                \end{equation*}
            \indent As $\textbf{u}$, $\textbf{v}$, $\textbf{w}$ are orthonormal, they can form a new group of base vectors. Matrix $\textbf{R}_{uvw}$ and its inverse can be perceived as the transform of the \emph{rotation} between different coordinate systems. Especially, $\textbf{R}_{uvw}$ means converting from \emph{current} base to \emph{new} base while its inverse means converting from \emph{new} base to \emph{current} base.\\

            Think like above, we can easily get the formula for rotation around $x$-axis, $y$-axis, $z$-axis:
            \newline
            \begin{equation*}
                \textbf{R}_{x}(\phi) = 
                \begin{pmatrix}
                    1 & 0 & 0 & 0\\
                    0 & \rm{cos\phi} & \rm{-sin\phi} & 0\\
                    0 & \rm{sin\phi} & \rm{cos\phi} & 0\\
                    0 & 0 & 0 & 1 
                \end{pmatrix}
            \end{equation*}
            \newline
            \begin{equation*}
                \textbf{R}_{y}(\phi) = 
                \begin{pmatrix}
                    \rm{cos\phi} & 0 & \rm{sin\phi} & 0\\
                    0 & 1 & 0 & 0\\
                    \rm{-sin\phi} & 0 & \rm{cos\phi} & 0 \\
                    0 & 0 & 0 & 1
                \end{pmatrix}
            \end{equation*}
            \newline
            \begin{equation*}
                \textbf{R}_{z}(\phi) = 
                \begin{pmatrix}
                    \rm{cos\phi} & \rm{-sin\phi} & 0 & 0\\
                    \rm{sin\phi} & \rm{cos\phi} & 0 & 0\\
                    0 & 0 & 1 & 0\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
            \end{equation*}
            $$\textbf{R}^{-1}_i(\phi) = \textbf{R}_i(-\phi) = \textbf{R}_i^T(\phi)$$
            \indent Attention: every rotation matrix is orthogonal and has a determinant of 1. And the \emph{trace} is constant independent of axis:$$tr(\textbf{R}) = 1 + 2\rm{cos\phi}$$
        
            \subsubsection{Scaling}
            \begin{equation*}
                \textbf{S}(\textbf{s}) = \textbf{S}(s_x\ s_y\ s_z) =
                \begin{pmatrix}
                    s_x & 0 & 0 & 0\\
                    0 & s_y & 0 & 0\\
                    0 & 0 & s_z & 0\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
            \end{equation*}
            $$\textbf{S}^{-1}(\textbf{s}) = \textbf{S}(1/s_x\ 1/s_y\ 1/s_z)$$
            \indent The scaling operation is called \emph{uniform} if $s_x = s_y = s_z$ and \emph{nonuniform} otherwise.\\
            \newpage
            For uniform scaling operation, there are two forms of matrix:
            \begin{equation*}
                \textbf{S} = 
                \begin{pmatrix}
                    s & 0 & 0 & 0\\
                    0 & s & 0 & 0\\
                    0 & 0 & s & 0\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
                ,\quad\quad
                \textbf{S}^{'} = 
                \begin{pmatrix}
                    1 & 0 & 0 & 0\\
                    0 & 1 & 0 & 0\\
                    0 & 0 & 1 & 0\\
                    0 & 0 & 0 & 1/s
                \end{pmatrix}
            \end{equation*}
            \indent A negative on one or three of the components of $\textbf{s}$ gives a \emph{reflection matrix}, or \emph{mirror matrix}. If only two scale factors are negative, then we will rotate $\pi$ radians.If the matrix has a negative determinants, then it's reflective.

            \indent If you want scale the axis of the orthonormal, right-oriented vectors $\textbf{f}_x$, $\textbf{f}_y$ and $\textbf{f}_z$. You can first rotate the current axis to the new one, do the scaling and then rotate it back. That is:\newline
            \begin{equation*}
                \textbf{F} = 
                \begin{pmatrix}
                    \textbf{f}_x & 0\\
                    \textbf{f}_y & 0\\
                    \textbf{f}_z & 0\\
                    0 & 1
                \end{pmatrix}
            \end{equation*}
            $$\textbf{S}^{'} = \textbf{F}^T\textbf{S}(\textbf{s})\textbf{F}$$
        
            \subsubsection{Shearing}
            $\textbf{H}_{ij}$ means $i$ coordinate is changed by the shearing matrix and $j$ coordinate does the shearing. For example:\newline
            \begin{equation*}
                \textbf{H}_{xz}(s)=
                \begin{pmatrix}
                    1 & 0 & s & 0\\
                    0 & 1 & 0 & 0\\
                    0 & 0 & 1 & 0\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
            \end{equation*}
            $$\textbf{H}_{ij}^{-1}(s) = \textbf{H}_{ij}(-s)$$
            \indent Another form:\newline
            \begin{equation*}
                \textbf{H}_{xy}^{'}(s,\ t) =
                \begin{pmatrix}
                    1 & 0 & s & 0\\
                    0 & 1 & t & 0\\
                    0 & 0 & 1 & 0\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
            \end{equation*}
            $$\textbf{H}_{xy}^{'}(s,\ t) = \textbf{H}_{xz}(s)\textbf{H}_{yz}(t)$$
            \indent Since every shearing matrix has a determinant of 1, this is a volume preserving transformation.
        
            \subsubsection{Concatenation of Transforms}
                It's associative.
                $$\textbf{T}\textbf{R}\textbf{S}\textbf{p} = (\textbf{T}(\textbf{R}(\textbf{S}\textbf{p}))) = (\textbf{T}\textbf{R})(\textbf{S}\textbf{p})$$
                \newpage
        
            \subsubsection{The Rigid-Body Transformation}
                \emph{Rigid-body transformation} refers to that consists of only rotations and translations.\newline
                \begin{equation*}
                    \textbf{X} = \textbf{T}\textbf{R}=
                    \begin{pmatrix}
                        r_{00} & r_{01} & r_{02} & t_x\\
                        r_{10} & r_{11} & r_{12} & t_y\\
                        r_{20} & r_{21} & r_{22} & t_z\\
                        0 & 0 & 0 & 1
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        \textbf{R} & \textbf{t}\\
                        \textbf{0}^T & 1
                    \end{pmatrix}
                \end{equation*}
                \newline
                \begin{equation*}
                    \textbf{X}^{-1} = 
                    \begin{pmatrix}
                        \textbf{R}^T & -\textbf{R}^T\textbf{t}\\
                        \textbf{0}^T & 1
                    \end{pmatrix}
                \end{equation*}
            
            \subsubsection{Normal Transformation}
                The original Transform can't directly applied to normals, but it can be applied to tangent vector.The correct matrix for normals are actually the \emph{transpose of the original matrix's adjoint}.Attention, the normal needs normalization after the transform.\newline
                \indent Prove when original matrix's inverse exists:\newline
                $$\textbf{n}^T\textbf{t} = 0$$
                $$\textbf{n}^T(\textbf{M}^{-1}\textbf{M})\textbf{t} = 0$$
                $$(\textbf{n}^T\textbf{M}^{-1})(\textbf{M}\textbf{t}) = 0$$
                $$(\textbf{n}^T\textbf{M}^{-1})\textbf{t}_M = 0$$
                \indent As normal is always perpendicular to tangent:\newline
                $$\textbf{n}_M^T = \textbf{n}^T\textbf{M}^{-1}$$
                \indent So the real transform matrix is:\newline
                $$\textbf{M}^{'} = (\textbf{M}^{-1})^T=({\frac{\textbf{M}^{*}}{|\textbf{M}|})^T} \to \textbf{M}^{'} = ({\textbf{M}^{*}})^T$$
                \indent There some optimization. As the normal is a vector, translation will not affect it. In affine transformation, they will not change the $w$ component. So we only need to calculate the adjoint of the upper-left 3 $\times$ 3 component. Often even this adjoint is unnecessary: Consider the concatenation of translations, \emph{uniform} scaling and rotations. Rotations will remain same as the inverse of rotation matrix is it's transpose, which will cancel out,  and other two have no effect on normals.\\ 
                \indent Normalization is not always needed as long as the $|\textbf{M}| = 1$. It means that the concatenation involves a scaling that $|\textbf{S}| \ne 1$\newpage 
            
            \subsubsection {Computations of Inverses}
                Something about  eigenvalues and singular value decomposition.\\
                Eigenvalue:
                $$\textbf{A}\textbf{a} = \lambda\textbf{a}$$
                \indent Factor $\lambda$ is called eigenvalue, and $\textbf{a}$ is called eigenvector. To calculate eigenvalue:
                $$(\textbf{A} - \lambda\textbf{I})\textbf{a} = 0$$
                $$|\textbf{A} - \lambda\textbf{I}| = 0$$
                \indent When \textbf{A} is \emph{symmetric matrices} ($\textbf{A} = \textbf{A}^T$), the decomposition is quite simple for matrix \textbf{A}: 
                $$\textbf{A} = \textbf{QD}\textbf{Q}^T$$
                Where \textbf{Q} is an orthogonal matrix and \textbf{D} is a diagonal matrix. The columns of \textbf{Q} are the eigenvector of \textbf{A} and the diagonal elements of \textbf{D} are the eigenvalues of \textbf{A}.\\
                
                There is another generalization of the symmetric eigenvalue decomposition for non-symmetric matrices:\emph{singular value decomposition}(SVD).For matrix \textbf{A}: 
                    $$\textbf{A} = \textbf{USV}^T$$
                \indent \textbf{U},\textbf{V} are potentially different, orthogonal matrices whose column are known as \emph{singular vectors}. \textbf{S} is a diagonal matrix whose entries are \emph{singular value}. To calculate the singular value and \textbf{U}, \textbf{V}, take \textbf{U} as example:
                    $$\textbf{M} = \textbf{AA}^T = (\textbf{USV}^{T})(\textbf{USV}^T)^T = \textbf{USV}^T\textbf{V}\textbf{S}^T\textbf{U}^T = \textbf{U}\textbf{S}^2\textbf{U}^T$$
                \indent This is the form of eigenvalue decomposition for matrix \textbf{M}(which is a symmetric matrix) and we can get \textbf{U}, \textbf{S}. Set $\textbf{M} = \textbf{A}^T\textbf{A}$can work \textbf{V} out.\\
                
                This two kinds of decomposition stands for two kinds of transform decomposition, which can lead us to the inverse of the transformation(only consider the left upper 3 $\times 3$ part). 
                $$\textbf{A} = \textbf{RSR}^T$$ 
                \indent Where $\textbf{v}_1$, $\textbf{v}_2$, $\textbf{v}_3$ are the eigenvectors and $\lambda_1$, $\lambda_2$, $\lambda_3$ are the eigenvalues.
                \begin{enumerate}
                    \item  Rotate the coordinate to make $\textbf{v}_1$ $\textbf{v}_2$ $\textbf{v}_3$ become the coordinate axis.
                    \item Scale in $x$ and $y$ by $(\lambda_1\ \lambda_2\ \lambda_3)$
                    \item Rotate the coordinate to the original one.
                \end{enumerate}

                \indent It's similar for singular decomposition which form is: 
                $$\textbf{A} = \textbf{R}_2\textbf{S}\textbf{R}_1^T$$

                There are also some basic ways to compute inverse, and another thing is when dealing with vector, we only needs to calculate the left upper 3 $\times$ 3 matrix's inverse.
                \newpage

        \subsection{Special Matrix Transform Operations}
            \subsubsection{The Euler Transform}
                The idea to \emph{Euler transform} is pretty easy, as to decompose a rotation to the rotation around three different axis:
                $$\textbf{E}(h,\ p,\ r) = \textbf{R}_i(r)\textbf{R}_j(p)\textbf{R}_k(h)$$
                \indent However, when $p = \frac{\pi}{2}$, the \emph{gimbal lock} will happen as one degree of freedom is lost. Directly, it will lead to one of axis in the model space to be rounded twice. Mathematically, when $p$ is set to $\frac{\pi}{2}$, take one order as example: 
                \begin{equation*}
                    \textbf{E}(h,\ \frac{\pi}{2},\ r) =
                    \begin{pmatrix}
                        \rm{cos}(\emph{r} + \emph{h}) & 0 & \rm{sin}(\emph{r} + \emph{h}) \\
                        \rm{sin}(\emph{r} + \emph{h}) & 0 & \rm{-cos}(\emph{r} + \emph{h})\\
                        0 & 1 & 0
                    \end{pmatrix}
                \end{equation*}
                \indent Since the matrix is only depend on $(r + h)$, we can see on degree of freedom is lost. Meanwhile, the \emph{interpolation} of Euler transform is not interpolating the angle, which is its main weakness. 
                
            \subsubsection{Extracting Parameters from the Euler transform}
                \begin{equation*}
                    \textbf{F} = 
                    \begin{pmatrix}
                        f_{00} & f_{01} & f_{02}\\
                        f_{10} & f_{11} & f_{12}\\
                        f_{20} & f_{21} & f_{22}
                    \end{pmatrix}
                    = \textbf{R}_z(r)\textbf{R}_x(p)\textbf{R}_z(h) = \textbf{E}(h,\ p,\ r)
                \end{equation*}
                \newline
                \begin{equation*}
                    \textbf{F} = 
                    \begin{pmatrix}
                        \rm{cos}\emph{r}\rm{cos}\emph{h} - \rm{sin}\emph{r}\rm{sin}\emph{p}\rm{sin}\emph{h} & 
                        -\rm{sin}\emph{r}\rm{cos}\emph{p} & 
                        \rm{cos}\emph{r}\rm{sin}\emph{h} + \rm{sin}\emph{r}\rm{sin}\emph{p}\rm{cos}\emph{h} \\
                        \rm{sin}\emph{r}\rm{cos}\emph{h} + \rm{cos}\emph{r}\rm{sin}\emph{p}\rm{sin}\emph{h} &
                        \rm{cos}\emph{r}\rm{cos}\emph{p} &
                        \rm{sin}\emph{r}\rm{sin}\emph{h} - \rm{cos}\emph{r}\rm{sin}\emph{p}\rm{cos}\emph{h} \\
                        -\rm{cos}\emph{p}\rm{sin}\emph{h} &
                        \rm{sin}\emph{p} & 
                        \rm{cos}\emph{p}\rm{cos}\emph{h}
                    \end{pmatrix} 
                \end{equation*} 
                \newline
                $$\frac{f_{01}}{f_{11}} = -\rm{tan}\emph{r}\quad\quad \frac{f_{20}}{f_{22}} = -\rm{tan}\emph{h}$$\newline
                $$h = \textbf{atan2}(-f_{20}, -f_{22})$$  $$ p = arcsin{f_{21}}$$ $$r = \textbf{atan2}(-f_{10}, -f_{11})$$
                \indent In a special case, when $f_{10} = 0 $ implying $f_{21} = \pm 1$:\newline
                \begin{equation*}
                    \textbf{F} = 
                    \begin{pmatrix}
                        \rm{cos}(\emph{r} + \emph{h}) & 0 & \rm{sin}(\emph{r} + \emph{h}) \\
                        \rm{sin}(\emph{r} + \emph{h}) & 0 & \rm{-cos}(\emph{r} + \emph{h})\\
                        0 & \pm 1 & 0 
                    \end{pmatrix} 
                \end{equation*} 
                $$\emph{h} = 0 \quad\quad \emph{r} = \frac{f_{10}}{f_{00}}$$
        
            \subsubsection{Matrix Decomposition}
                \newpage
            \subsubsection{Rotation about an Arbitrary Axis} 
                Use the given vector $\text{r}$ to form orthonormal axis of base $\textbf{r}$, $\textbf{s}$, $\textbf{t}$. $\text{r}$ needs normalization first. Use the base to form matrix $\textbf{M}$ and do the transform.
                \begin{equation*} \bar{\textbf{s}} = 
                    \left\{
                    \begin{array}{lr}
                        (0, -r_z, r_y),\ if & |r_x| < |r_y|\  and\ |r_x| < |r_z| \\
                        (-r_z, 0, r_x),\ if & |r_y| < |r_x|\  and\ |r_y| < |r_z| \\
                        (-r_y, 0, r_x),\ if & |r_z| < |r_x|\  and\ |r_z| < |r_y| 
                    \end{array}
                    \right.
                \end{equation*} 
                $$\textbf{s} = \frac{\bar{\textbf{s}}}{\parallel{\bar{\textbf{s}}}\parallel} $$
                $$\textbf{t} = \textbf{r}\times\textbf{s}$$
                \begin{equation*}
                    \textbf{M} = 
                    \begin{pmatrix}
                        \textbf{r}^T \\
                        \textbf{s}^T \\
                        \textbf{t}^T
                    \end{pmatrix} 
                \end{equation*} 
                $$\textbf{X} = \textbf{M}^T\textbf{R}_{x}(\phi)\textbf{M}$$
                
                Another method for a normalized vector $\textbf{r}$ by $\phi$ radians: 
                \begin{equation*}
                    \textbf{X} = 
                    \begin{pmatrix}
                        \cos{\phi} + (1 - \cos{\phi})r_x^2 & (1 - \cos{\phi})r_xr_y - r_z\sin{\phi} & (1 - \cos{\phi}r_xr_z + r_y\sin{\phi} \\
                        (1 - \cos{\phi})r_xr_y + r_z\sin{\phi} & \cos{\phi} + (1 - \cos{\phi})r_y^2 & (1 - \cos{\phi}r_yr_z - r_x\sin{\phi} \\
                        (1 - \cos{\phi})r_xr_z - r_y\sin{\phi} & (1 - \cos{\phi})r_yr_z + r_x\sin{\phi} & (\cos{\phi} + (1 - \cos{\phi})r_z^2
                    \end{pmatrix}
                \end{equation*} 
                \newpage
        
        \subsection{Quaternions}
            \subsubsection{Mathematical background}
            \emph{Quaternions} are much more straight forward than matrix and Euler transform in rotation and orientation.\\    
            Definition:\\
            $$\hat{\textbf{q}} = (\textbf{q}_v,\ \textbf{q}_w) = iq_x + jq_y + kq_z + q_w = \textbf{q}_v + q_w,$$
            $$\textbf{q}_v = iq_x + jq_y + kq_z + q_w = (q_x,\ q_y,\ q_z)$$
            $$i^2 = j^2 = k^2 = -1, jk = -kj = i, ki = -ik = j, ij = -ji = k$$
            \indent All operations of $\textbf{q}_v$ are the same as the vectors'.\\
            \indent Multiplication: 
            \begin{align*}
                \hat{\textbf{q}}\hat{\textbf{r}} &= (iq_x + jq_y + kq_z + q_w)(ir_x + jr_y + kr_z + r_w)\\
                    &= (\textbf{q}_v\times\textbf{r}_v + r_w\textbf{q}_v + q_w\textbf{r}_v,\ q_wr_w - \textbf{q}_v\cdot  \textbf{r}_v)
            \end{align*}
            \indent Addition:
                $$\hat{\textbf{q}} + \hat{\textbf{r}} = (\textbf{q}_v + \textbf{r}_v,\ q_w + r_w)$$
            \indent Conjugate:
                $$(\hat{\textbf{q}})^* = (-\textbf{q}_v,\ q_w)$$
            \indent Norm: 
                $$n(\hat{\textbf{q}}) = \sqrt{\hat{\textbf{q}}\hat{\textbf{q}}^*} =  \sqrt{\textbf{q}_v\cdot\textbf{q}_v + q_w^2} = \sqrt{q_x^2 + q_y^2 + q_z^2 + q_w^2}$$ 
            \indent Identity:
                $$\hat{\textbf{i}} = (\textbf{0}, 1)$$
            \indent Inverse: 
                $$\hat{\textbf{q}}^{-1} = \frac{\hat{\textbf{q}}^*}{n(\hat{\textbf{q}})^2}$$ 
                \indent Commutative: 
                $$\hat{\textbf{q}}s = s\hat{\textbf{q}} = (\textbf{0}, s)(\textbf{q}_v, q_w) = (s\textbf{q}_v, sq_w)$$
            \indent Conjugate rules: 
                $$(\hat{\textbf{q}}^*)^* = \hat{\textbf{q}}$$
                $$(\hat{\textbf{q}} + \hat{\textbf{r}})^* = \hat{\textbf{q}}^* + \hat{\textbf{r}}^*$$
                $$(\hat{\textbf{q}}\hat{\textbf{r}})^* = \hat{\textbf{r}}^*\hat{\textbf{q}}^*$$
            \indent Norm rules: 
                $$n(\hat{\textbf{q}}^*) = n(\hat{\textbf{q}})$$
                $$n(\hat{\textbf{q}}\hat{\textbf{r}}) = n(\hat{\textbf{q}})n(\hat{\textbf{r}})$$
            \indent Linearity: 
                $$\hat{\textbf{p}}(s\hat{\textbf{q}} + t\hat{\textbf{r}}) = s\hat{\textbf{p}}\hat{\textbf{q}} + t\hat{\textbf{p}}\hat{\textbf{r}}$$
                $$(s\hat{\textbf{p}} + t\hat{\textbf{q}})\hat{\textbf{r}} = s\hat{\textbf{p}}\hat{\textbf{r}} + t\hat{\textbf{q}}\hat{\textbf{r}}$$
            \indent Associativity:
                $$\hat{\textbf{p}}(\hat{\textbf{q}}\hat{\textbf{r}}) = (\hat{\textbf{p}}\hat{\textbf{q}})\hat{\textbf{r}}$$
            \indent Unit quaternion which is $n(\hat{\textbf{q}}) = 1$ with $\textbf{u}_q\cdot\textbf{u}_q = 1$: 
                $$\hat{\textbf{q}} = (\sin{\phi}\textbf{u}_q, \cos{\phi}) = \sin{\phi}\textbf{u}_q + \cos{\phi} = e^{\phi\textbf{u}_q}$$
                $$\log{(\hat{\textbf{q}})} = \log{(e^{\phi\textbf{u}_q})} = \phi\textbf{u}_q$$
                $$(\hat{\textbf{q}})^t = e^{t\phi\textbf{u}_q} = \sin{(\phi t)}\textbf{u}_q + \cos{(\phi t)}$$
        
            \subsubsection{Quaternion Transform}
                The \emph{unit quaternions} can represent any three-dimensional rotation. Put the four coordinates of a point or a vector $(p_x,\ p_y,\ p_z,\ p_w)^T$ into a quaternion $\hat{\textbf{p}}$. With the unit quaternion $\hat{\textbf{q}} = (\sin{\phi}\textbf{u}_q, \cos{\phi})$:
                $$\hat{\textbf{q}}\hat{\textbf{p}}\hat{\textbf{q}}^{-1}$$
                
                the vector or point will be rotated around axis $\textbf{u}_q$ by $\textbf{2$\phi$}$. Not that here $\hat{\textbf{q}}^{-1} = \hat{\textbf{q}}^{*}$. Any nonzero real multiple of $\hat{\textbf{q}}$ also represents the same transform.Extracting a quaternion from a matrix can return either $\hat{\textbf{q}}$ or $-\hat{\textbf{q}}$.

                The concatenation is similar to matrix: 
                $$\hat{\textbf{r}}(\hat{\textbf{q}}\hat{\textbf{p}}\hat{\textbf{q}}^{*})\hat{\textbf{r}}^{*} = (\hat{\textbf{r}}\hat{\textbf{q}})\hat{\textbf{p}}(\hat{\textbf{r}}\hat{\textbf{q}})^{*}$$
                
                The conversion from quaternion to matrix as $s = \frac{2}{n(\hat{\textbf{q}})}$:
                    \begin{equation*}
                        \textbf{M}^q = 
                        \begin{pmatrix}
                            1 - s(q_y^2 + q_z^2) & s(q_xq_y - q_wq_z) & s(q_xq_z + q_wq_y) & 0\\
                            s(q_xq_y + q_wq_z) & 1 - s(q_x^2 + q_z^2) & s(q_yq_z - q_xq_w) & 0\\
                            s(q_xq_z - q_yq_w) & s(q_yq_z + q_wq_x) & 1 - s(q_x^2 + q_y^2) & 0\\
                            0 & 0 & 0 & 1
                        \end{pmatrix}
                    \end{equation*}
                
                For unit quaternion especially: 
                \begin{equation*}
                    \textbf{M}^q = 
                    \begin{pmatrix}
                        1 - 2(q_y^2 + q_z^2) & 2(q_xq_y - q_wq_z) & 2(q_xq_z + q_wq_y) & 0\\
                        2(q_xq_y + q_wq_z) & 1 - 2(q_x^2 + q_z^2) & 2(q_yq_z - q_xq_w) & 0\\
                        2(q_xq_z - q_yq_w) & 2(q_yq_z + q_wq_x) & 1 - 2(q_x^2 + q_y^2) & 0\\
                        0 & 0 & 0 & 1
                    \end{pmatrix}
                \end{equation*}

                To extract quaternion from matrix:
                $$\textbf{M}_{21}^q - \textbf{M}_{12}^q = 4q_wq_x$$
                $$\textbf{M}_{02}^q - \textbf{M}_{20}^q = 4q_wq_y$$
                $$\textbf{M}_{10}^q - \textbf{M}_{01}^q = 4q_wq_z$$
                $$tr(\textbf{M}^q) = 4q_w^2$$
                
                It's better to first find out the biggest entry for numerical precision.
                \newpage
                \emph{Spherical linear interpolation} is used to interpolated between two quaternions as quaternion can be seen as a point on a four-dimensional sphere. The original form for any interpolation is:
                $$\textbf{s}(\textbf{q}, \textbf{r}, t) = \frac{\sin{(1 - t)\phi}}{\sin{\phi}}\textbf{q} + \frac{\sin{t\phi}}{\sin{\phi}}\textbf{r}$$

                \textbf{q}, \textbf{r} are points on the sphere and $\phi$ is the angle between two radiuses towards points. For quaternion it can also be considered as: 
                $$\hat{\textbf{s}}(\hat{\textbf{q}}, \hat{\textbf{r}},\ t) = (\hat{\textbf{r}}\hat{\textbf{q}}^{-1})^t\hat{\textbf{q}}$$
                
                in quaternions, $\phi$ can be computed through $\cos{\phi} = q_xr_x + q_yr_y + q_zr_z + q_wr_w$ The interpolate path is arc on the sphere.\\
                
                The transform from one vector \textbf{s} to \textbf{t}: First \textbf{s} and \textbf{t} should be normalized. 
                $$\textbf{u} = \textbf{s} \times \textbf{t}$$
                $$e = \textbf{s} \cdot \textbf{t} = \cos{2\phi}$$
                $$\parallel\textbf{u}\parallel = \parallel\textbf{s}\times\textbf{t}\parallel = \sin{2\phi}$$
                $$\hat{\textbf{q}} = (\frac{\sin{\phi}}{\sin{2\phi}}\textbf{u}, \cos{\phi}) = (\frac{1}{\sqrt{2(1 + e)}}(\textbf{s} \times \textbf{t}), \frac{\sqrt{2(1 + e)}}{2})$$
                \indent For matrix form(this form can avoid numerical issue): 
                \begin{equation*}
                    \textbf{R}(\textbf{s},\ \textbf{t}) = 
                    \begin{pmatrix}
                        e + hu_x^2 & hu_xu_y - u_z & hu_xu_z + u_y & 0 \\
                        hu_xu_y + u_z & e + hu_y^2 & hu_yu_z - u_x & 0 \\
                        hu_xu_z - u_y & hu_yu_z + u_x & e + hu_z^2 & 0 \\
                        0 & 0 & 0 & 1
                    \end{pmatrix}
                \end{equation*}
                $$ h  = \frac{1 - \cos{2\phi}}{\sin^2{2\phi}} = \frac{1 - e}{\textbf{u} \cdot \textbf{u}} = \frac{1}{1 + e}$$
                \indent For $2\phi \approx 0$, transform matrix is identity; for $2\phi \approx \pi$, we can choose one of the vector as the axis to rotate by.
                \newpage 

        \subsection{Vertex Blending}
            Consider the transformation of the arms made up of forearm and upper arm. When they are both \emph{rigid-body}, the joint will be presented as the overlap, which is not flexible. \emph{stitching} is the technique that the join is an elastic skin on which vertex may have different transforms of the forearm or upper arm. One implementation is that keep the transforms the same in a triangle. 
            \emph{Vertex blending}, which is also called \emph{skinning}, \emph{enveloping}, and \emph{skeleton-subspace deformation}, the object has a skeleton of \emph{bones} and \emph{skin} referring to whole mesh. Every vertex will be affected by more than one bones thus having many different transforms. To blend all these transforms, in mathematical background:
                $$\textbf{u}(t) = \sum_{i = 0}^{n - 1}w_i\textbf{B}_i(t)\textbf{M}_i^{-1}\textbf{p},\quad \rm{where}\quad \sum_{i = 0}^{n - 1}w_i = 1,\quad w_i > 0$$
            \indent \textbf{u} is the final output that change with time $t$; $\textbf{M}_i$ is the  transform from the bone's coordinate space to the world space; $\textbf{B}_i$ is the transform in the world space; $\textbf{p}$ stands for the vertex.\\
            
            In implementations, $\sum{i = 0}^{n - 1}w_i$ can be out of range [0, 1] for specific blending algorithm like \emph{morph targets}. \emph{Dual quaternions} are employed to resolve unwanted folding, twisting and self-intersection.
        \subsection{Morphing}
            \emph{Morphing} uses linear interpolation to represent frames between key frames. After we find the one-to-one \emph{vertex correspondence} which may be very difficult, the interpolation is obvious enough:
                $$\textbf{m} = (1 - s)\textbf{p}_0 + s\textbf{p}_1,\quad s = \frac{t - t_0}{t_1 - t_0}$$
            \indent \emph{Morph targets}, or \emph{blend shapes} is used to interpolate between shapes. Consider we have a initial shape $\textbf{N}$ and other shape models $\textbf{P}_i$. $\textbf{D}_i = \textbf{P}_i - \textbf{N}$ stands for data describing the differentials between models and initial. An morphed model \textbf{M} can be obtained by: 
                $$\textbf{M} = \textbf{N} + \sum_{i = 0} ^ {n - 1}w_i\textbf{D}_i$$
            \indent $w_i$ can be negative symbolizing a inverse change of the model.
            \newpage
        
        \subsection{Projection}
            Setting: we are looking at negative $z$-axis as DirectX.
            \subsubsection{Orthographic Projection}
                The simplest one with intervals on $z$-axis from $n$(near plane) to $f$(far plane) ($n > f$)
                \begin{equation*}
                    \textbf{P}_o = 
                    \begin{pmatrix}
                        1 & 0 & 0 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 0 & 0 \\
                        0 & 0 & 0 & 1 
                    \end{pmatrix} 
                \end{equation*} 
            \indent Often the orthographic projection is expressed in terms of a AABB(\emph{Axis-Aligned Bounding Box}) with it's minimum corner$(l, b, n)$ and maximum $(r, t, f)$. The transform involves first translate it to the origin and then scale it to make it in the canonical view volume. For OpenGL, the minimum corner of canonical view volume is$(-1, -1, -1)$ and maximum corner is $(1, 1, 1)$ while the DirectX's bounds are $(-1, -1, 0)$ and $(-1, -1, 1)$.\\
            \indent Theoretically: 
                \begin{align*}
                    \textbf{P}_o = \textbf{S}(\textbf{s})\textbf{T}(\textbf{t}) 
                    &= 
                    \begin{pmatrix}
                        \frac{2}{r - l} & 0 & 0 & 0\\
                        0 & \frac{2}{t - b} & 0 & 0\\
                        0 & 0 & \frac{2}{f - n} & 0\\
                        0 & 0 & 0 & 1
                    \end{pmatrix}
                    \begin{pmatrix}
                        1 & 0 & 0 & -\frac{l + r}{2}\\
                        0 & 1 & 0 & -\frac{t + b}{2}\\
                        0 & 0 & 1 & -\frac{f + n}{2}\\
                        0 & 0 & 0 & 1 
                    \end{pmatrix}\\
                    &=
                    \begin{pmatrix}
                        \frac{2}{r - l} & 0 & 0 & -\frac{r + l}{r - l}\\
                        0 & \frac{2}{t - b} & 0 & -\frac{t + b}{t - b}\\
                        0 & 0 & \frac{2}{f - n} & -\frac{n + f}{f - n}\\
                        0 & 0 & 0 & 1
                    \end{pmatrix}
                \end{align*}
            \indent For DirectX, we should apply an additional transform for different $z$ intervals:
            \begin{align*}
                \textbf{P}_{o[0, 1]}
                &= 
                \begin{pmatrix}
                    1 & 0 & 0 & 0\\
                    0 & 1 & 0 & 0\\
                    0 & 0 & 0.5 & 0.5\\
                    0 & 0 & 0 & 1 
                \end{pmatrix}
                \begin{pmatrix}
                    \frac{2}{r - l} & 0 & 0 & -\frac{r + l}{r - l}\\
                    0 & \frac{2}{t - b} & 0 & -\frac{t + b}{t - b}\\
                    0 & 0 & \frac{2}{f - n} & -\frac{n + f}{f - n}\\
                    0 & 0 & 0 & 1
                \end{pmatrix}\\
                &=
                \begin{pmatrix}
                \frac{2}{r - l} & 0 & 0 & -\frac{r + l}{r - l}\\
                0 & \frac{2}{t - b} & 0 & -\frac{t + b}{t - b}\\
                0 & 0 & \frac{1}{f - n} & -\frac{n}{f - n}\\
                0 & 0 & 0 & 1
                \end{pmatrix}
            \end{align*}
            \indent As a left-handed projection is used after projection, a reflect transform is needed:\\
            \begin{equation*}
                \textbf{M} =
                \begin{pmatrix} 
                    1 & 0 & 0 & 0 \\
                    0 & 1 & 0 & 0 \\ 
                    0 & 0 & -1 & 0\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
            \end{equation*}
        \newpage 

        \subsubsection{Prospective Projection}
            \emph{Prospective projection} project point $\textbf{p}$ to point $\textbf{q}$ on plane $-d, d > 0$:
                $$ \frac{q_x}{p_x} = \frac{-d}{p_z} \iff q_x = \frac{-dp_x}{p_z}$$
            
            in matrix form: 
                \begin{equation*}
                    \textbf{P}_p = 
                    \begin{pmatrix}
                        1 & 0 & 0 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 1 & 0 \\
                        0 & 0 & -\frac{1}{d} & 0
                    \end{pmatrix}
                \end{equation*}
                \begin{equation*} 
                    \textbf{q} = \textbf{P}_p\textbf{p} = 
                    \begin{pmatrix}
                    1 & 0 & 0 & 0 \\
                    0 & 1 & 0 & 0 \\
                    0 & 0 & 1 & 0 \\
                    0 & 0 & -\frac{1}{d} & 0
                    \end{pmatrix}
                    \begin{pmatrix}
                        p_x\\
                        p_y\\
                        p_z\\
                        1
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        p_x\\
                        p_y\\
                        p_z\\
                        -\frac{p_z}{d}
                    \end{pmatrix}
                    \iff
                    \begin{pmatrix}
                        -\frac{dp_x}{p_z}\\
                        -\frac{dp_y}{p_z}\\
                        -d\\
                        1
                    \end{pmatrix}
                \end{equation*}
                
            Like orthogonal projection, rather than projecting onto a plane, we transform the \emph{view frustum} to canonical view volume with $(l, r, b, t, n, f)$(minimum corner: $(l, b, n)$ and maximum corner $(r, t, n)$ on near plane). We can concatenate the transform from view frustum to rectangular and orthogonal transform. The following transform $\textbf{P}_t$ guarantees that all points on the near plane remain the same. 
            \begin{align*}
                \textbf{P}_p &= \textbf{P}_o\textbf{P}_t\\
                &= \begin{pmatrix}
                    \frac{2}{r - l} & 0 & 0 & -\frac{r + l}{r - l}\\
                    0 & \frac{2}{t - b} & 0 & -\frac{t + b}{t - b}\\
                    0 & 0 & \frac{2}{f - n} & -\frac{n + f}{f - n}\\
                    0 & 0 & 0 & 1
                \end{pmatrix}
                \begin{pmatrix}
                    n & 0 & 0 & 0\\
                    0 & n & 0 & 0\\
                    0 & 0 & n + f & -2fn\\
                    0 & 0 & 1 & 0
                \end{pmatrix}\\
                &=\begin{pmatrix}
                    \frac{2n}{r - l} & 0 & -\frac{r + l}{r - l} & 0\\
                    0 & \frac{2n}{t - b} & -\frac{t + b}{t - b} & 0\\
                    0 & 0 & \frac{f + n}{f - n} & -\frac{2fn}{f - n}\\
                    0 & 0 & 1 & 0
                \end{pmatrix}
            \end{align*} 

            For OpenGL, as $n^{'} = -n$, $f^{'} = -f$:
            \begin{equation*}
                \textbf{P}_{OpenGL} = \textbf{P}_o\textbf{S}(1,\ 1,\ -1) = 
                \begin{pmatrix}
                    \frac{2n^{'}}{r - l} & 0 & -\frac{r + l}{r - l} & 0\\
                    0 & \frac{2n^{'}}{t - b} & -\frac{t + b}{t - b} & 0\\
                    0 & 0 & \frac{f^{'} + n^{'}}{f^{'} - n^{'}} & -\frac{2f^{'}n^{'}}{f^{'} - n^{'}}\\
                    0 & 0 & 1 & 0
                \end{pmatrix}
            \end{equation*}
        
            For DirectX, as it implements left-handed coordinates and $z$-interval in [0, 1]:
            \begin{equation*}
                \textbf{P}_{p[0,1]} = \textbf{P}_{o[0, 1]}\textbf{S}(1,\ 1,\ -1) = 
                \begin{pmatrix}
                    \frac{2n^{'}}{r - l} & 0 & -\frac{r + l}{r - l} & 0\\
                    0 & \frac{2n^{'}}{t - b} & -\frac{t + b}{t - b} & 0\\
                    0 & 0 & \frac{f^{'}}{f^{'} - n^{'}} & -\frac{f^{'}n^{'}}{f^{'} - n^{'}}\\
                    0 & 0 & 1 & 0
                \end{pmatrix}
            \end{equation*}
            \newpage

\section{Visual Appearance}
    \subsection{Visual Phenomena} 
        Light is emitted by light sources, scattered by object and finally absorbed by a sensor.
    \subsection{Light Source}
        Light discussed here are \emph{directional lights}, presented by \emph{light vector} \textbf{l}. $\textbf{l}$ has a length of 1 and always point to the \emph{opposite} of the direction that light travels. The science measuring of light is \emph{radiance}. The emission quantity of a direction light source is \emph{irradiance}, which stands for the power through a unit area perpendicular to \textbf{l}, describing the brightness of an area(like surface). We represent irradiance as an RBG vector as light has color. Environmental light is called \emph{ambient light}.
            
        The surface irradiance is equal to irradiance measured perpendicular to \textbf{l} times the cosine of the angle $\theta_i$ between \textbf{l} and normal \textbf{n}.Irradiance is proportional to the \emph{density} of light and inversely proportional to the \emph{distance} between rays. We use $E$ to stand for irradiance:
        $$E = E_L\bar{\cos{\theta_i}} = E_L \max(\textbf{l}\cdot\textbf{n},\ 0)$$
        
        In reality the direction of light is arbitrary, requiring a \emph{light meter} to measure. For multiple light sources: 
        $$E = \sum_{k = 1}^{n}E_{L_k}\bar{\cos{\theta_{i_k}}}$$
    
    \subsection{Material}
        Fundamentally, all light-matter interaction are the result of two phenomena:\emph{scattering} and \emph{absorption}. Scattering happens when light encounters any kind of optical discontinuity, usually interface between surface and air, involving \emph{reflection} and \emph{refraction} (\emph{transmission}), which change the direction without changing the amount. Absorption happens inside matter and causes some of the light to be converted into another kind of energy and disappear, which reduces the amount but don't affect its direction. 
        
        In opaque objects, surface shading equation is divided to two parts: \emph{specular term} representing the light that was reflected at the surface, and \emph{diffuse term} representing light which has undergone transmission, absorption and scattering. Scattering is different with reflection here as reflection obeys the law of reflection. To characterize by a shading equation, we need to represent the amount and direction of \emph{out going light} based on the amount and direction of the \emph{incoming light}.

        Incoming illumination is measured as surface irradiance. The outgoing light is measured as \emph{exitance} with symbol \textbf{M}. Light-matter interaction is linear. The ratio of exitance and irradiance is between 0 to 1 in opaque objects and differentiates in colors. There are represented as RGB vector called the \emph{surface color} \textbf{c}.It can be divided into two terms \emph{specular color} $\textbf{c}_{\rm{spec}}$ and \emph{diffuse color} $\textbf{c}_{\rm{diff}}$. They are dependent on the its composition.
        
        In this chapter we assume diffuse term has no directionality. The directional distribution of the specular term depends on the surface smoothness, which is also involved in shading. Smoothness can either be a parameter in shading equation, or be made into model or texture, depending on the situation. 

    \subsection{Sensor}
        Sensors to form images should include a light-proof enclosure with a single small \emph{aperture}(opening) that restricts the directions of light. The combination of enclosure, lens and aperture causes the sensor to be \emph{directionally specific}. That is, the sensor measure average \emph{radiance}($\textbf{L}$), the density of light flow per unit area per incoming direction, rather than average irradiance, the density of light flow per unit area from all incoming direction. Radiance can used to describe the brightness and color of a ray of light. 

        In the model, each sensor only measure a single radiance sample which is along a ray goes through the sensor and the center of perspective projection. The detection of the sensor is replaced by the shader equation evaluation to evaluate radiance. This ray in the equation is represented as \emph{view vector} $\textbf{v}$ whose length is set to be 1. After the evaluation, the transform between radiance and signal is required. The physical sensors measure the \emph{average} value of the radiance over their area, over incoming directions focused by the lens, and over a time interval. 
    
    \subsection{Shading}
        \emph{Shading} is the process of using an equation to compute the outgoing radiance $\textbf{L}_o$ along the view ray, $\textbf{v}$, based on the material properties and light sources. In this chapter we will introduce a simple shading model. 
            $$M_{\rm{diff}} = \textbf{c}_{\rm{diff}} \otimes E_L\bar{\cos{\theta_i}}$$
            $$L_{\rm{diff}} = \frac{M_{\rm{diff}}}{\pi} = \frac{\textbf{c}_{\rm{diff}}}{\pi} \otimes E_L\bar{\cos{\theta_i}}$$
        
        This type of shading term is also called \emph{Lambertian}.In real time shading we will fold $\frac{1}{\pi}$ into $E_L$. 

        Similarly, for specular term: 
            $$M_{\rm{spec}} = \textbf{c}_{\rm{spec}} \otimes E_{L}\bar{\cos{\theta_i}}$$
        As the specular term is directional, we introduce \emph{half vector} $\textbf{h}$(some of the explanation is in chapter 7):
            $$\textbf{h} = \frac{\textbf{l} + \textbf{v}}{\parallel \textbf{l} + \textbf{v} \parallel}$$
            \begin{align*} L_{\rm{spec}}(\textbf{v}) &= \frac{m + 8}{8\pi}\bar{\cos^m{\theta_h}}M_{\rm{spec}}\\
             &= \frac{m + 8}{8\pi}\bar{\cos^m{\theta_h}}\textbf{c}_{\rm{spec}} \otimes E_{L}\bar{\cos{\theta_i}}
            \end{align*}
        
        where $\theta_h$ is the angle between $\textbf{h}$ and $\textbf{n}$.
        
        The total outgoing radiance $\textbf{L}_o$:
        $$\textbf{L}_o(\textbf{v}) = (\frac{\textbf{c}_{\rm{diff}}}{\pi} + \frac{m + 8}{8\pi}\bar{\cos^m{\theta_h}}\textbf{c}_{\rm{spec}}) \otimes E_L\bar{\cos{\theta_i}}$$
        
        which is quite similar to Bling-Phong equation.
        $$\textbf{L}_o(\textbf{v}) = (\bar{\cos{\theta_i}}\textbf{c}_{\rm{diff}} + \bar{\cos^m{\theta_h}}\textbf{c}_{\rm{spec}}) \otimes B_L$$
        
        \subsubsection{Implementing the Shading Equation} 
            For multiple light sources: 
            $$\textbf{L}_o(\textbf{v}) = \sum_{k = 1}^n((\frac{\textbf{c}_{\rm{diff}}}{\pi} + \frac{m + 8}{8\pi}\bar{\cos^m{\theta_{h_k}}}\textbf{c}_{\rm{spec}}) \otimes E_L\bar{\cos{\theta_{i_k}}})$$
            
            When implemented into shader, the computations need to be divided according to their \emph{frequency of evaluation}. The lowest frequency of evaluation is \emph{per-model}: Expressions that are constant over the entire model can be evaluated once, and the result passed to the graphics API. \emph{Per-primitive} can be performed by geometry shader if the result of computation is constant over each of the primitives comprising the model. In \emph{per-vertex} evaluation, where the evaluation is performed in vertex shader and passed to pixel shader. The highest frequency of evaluation is \emph{per-pixel}, which the evaluation is performed in pixel shader. We assume all material properties are same across the entire mesh. 

            Separating out the \emph per-model sub-expressions, $\textbf{K}_d$ and $\textbf{K}_s$ will be evaluated in the application: 
            $$\textbf{L}_o(\textbf{v}) = \sum_{k = 1}^n((\textbf{K}_d + \textbf{K}_s\bar{\cos^m{\theta_{h_k}}}) \otimes E_L\bar{\cos{\theta_{i_k}}})$$
            
            The view vector $\textbf{v}$ can be computed from the surface position $\textbf{p}$ to the view position $\textbf{p}_v$: 
            $$ \textbf{v} = \frac{\textbf{p}_v - \textbf{p}}{\parallel \textbf{p}_v - \textbf{p} \parallel}$$ 
            
            The normal $\textbf{n}$ derives from the vertex normal as triangle mesh is used to represent underlying curved structure with each vertex has normals per triangle. 
            
            Now we can create \textbf{Shade()} function. Per-primitives evaluation (also called \emph{flat shading} is not desirable as vertex normals involved. Per-vertex evaluation, known as \emph{Gouraud Shading}, passes normals and positions to the function and get an interpolated result passed to pixel shader and the result is directly written to the output. It's reasonable for matte surface but have noticeable artifacts on specular surface. Per-pixel, known as \emph{Phong shading}, uses vertex shader to interpolate normals and positions, then passed by the pixel to \textbf{Shader()}. It has no interpolation artifacts while costly. Solution to it is to adopt hybrid approach where some evaluations are per-vertex and some are per-pixel.

            Implementing a shading equation is a matter of deciding what parts can be simplified, how frequently to compute various expressions, and how the user will be able to modify and control the appearance.
            \newpage
         
        \subsection{Aliasing and Anti-aliasing}            \subsubsection{Sampling and Filtering Theory}
                {\footnotesize{Continuous image}} $\xrightarrow{\emph{sampling}}$ {\footnotesize{sampled signal}} $\xrightarrow{\emph{reconstruction}}$ {\footnotesize{reconstructed signal}}
            \newline

            When sampling is done, aliasing can occur when samples are taken in a series of time steps, which is called \emph{temporal aliasing}. It's because signal is sampled at a too low frequency.According to \emph{sample theory}, in order to sample properly, the sampling frequency has to be more than twice the maximum frequency, and the sampling frequency is called the \emph{nyquist rate} or \emph{Nyquist limit}. This also implies that the signal has to be \emph{bandlimited}. However, an three-dimensional scene is normally never bandlimited when rendered with point samples. But there is signal that is bandlimited like textures.

            Reconstruction from sampling requires for \emph{filtering}. Note that the area of filter should always be 1, or reconstructed signal can appear to grow or shrink. \emph{Box filter} is used as the simplest filter. \emph{Tent filter} is another one implementing interpolating between nearby samples. 
            
            In order to get better continuity, \emph{low-pass filter} is introduced. The \emph{frequency component} of a signal is a sine wave: $\sin{(2\pi f)}$, where $f$ is the frequency of that component. A low-pass filter removes all frequency components with frequencies higher than a certain frequency defined by the filter, removing sharp features of the signal. A ideal low-pass filter is sinc filter:
                $$\rm{sinc}(x) = \frac{\sin{\pi x}}{\pi x}$$
            
            In fact, the sinc filter eliminates all sine waves with frequencies higher than $\frac{1}{2}$ sampling rate, perfect for sampling rate at 1.0. More generally, assume the sampling frequency is $f_s$, the perfect filter is $\rm{sinc}(f_s x)$, which eliminates all frequencies  higher than $\frac{f_s}{2}$. However, as the filter width is infinite and is also negative at times, it's rarely useful in practice. In practice, we often use filters similar to sinc filter with limited pixels they influence. When negative filter value are undesirable or impractical, filters with no negative lobes.

            After reconstruction, we need \emph{resampling} to display signal. Resampling is used to magnify or minify a sampled signal. Originally all sampled points are on integer coordinates. For new sampled points with interval $a$ uniformly, if $a > 1$ \emph{minification} (\emph{downsampling}) take place, and for $a < 1$, \emph{magnification} (\emph{upsampling}) occurs. For magnification, it's pretty straight forward to sample from a perfectly reconstructed, continuous signal. For minification, it's better to use $\rm{sinc}(\frac{x}{a})$ to get reconstructed signal for resampling in order to blur the continuous signal.

            \subsubsection{Screen-Based Algorithm}
                Some anti-aliasing schemes are focused on particular primitives. There are two special cases: texture aliasing and line aliasing. For line aliasing, one method is to treat the line as quadrilateral one pixel wide that is blended with it's background; another is to consider it an infinitely thin, transparent object with a halo; third is to render the line as an anti-aliased texture. Hardware solution is dedicated to rapid, high-quality rendering. 

                One problem of aliasing is low sampling rate, which can be better when introducing more samples in the cell. The general strategy of screen-based anti-aliasing schemes is to use a sample pattern for the screen and then weight and sum the samples to produce a pixel color, $\textbf{p}$: 
                    $$\textbf{P}(x, y) = \sum_{i = 1}^nw_i\textbf{c}(i,x,y),\quad \sum_{i = 1}^nw_i = 1$$
                
                    Where the sample is taken on the screen grid is different for each sample, and optionally the sampling pattern can vary from pixel to pixel. As sampling point is point in real time rendering system, $\textbf{c}$ function can be thought of first retrieve position of the point and then the color. For $w_i$ in most design is set to be constant, i.e., $w_i = \frac{1}{n}$. The simplest anti-aliasing is a single sample at the center of pixel. 

                    Anti-aliasing algorithms that compute more than one full sample per pixel are called \emph{supersampling} (or \emph{oversampling}) methods. \emph{Full-scene anti-aliasing} (FSAA) renders the scene at a higher resolution and then average neighboring samples to create an image. It's costly but simple. Other, low quality FSAA is to sample at twice the rate on only one screen axis. 

                    A related method is the \emph{accumulation buffer}. This method instead uses a buffer that has the same resolution with more color bits. To obtain an 2$\times$2 sampling, four images are generated with the moved half a pixel in the screen $x$- or $y$- axis. They can be used to create effects such as \emph{motion blurring} and \emph{depth of field} (where objects not at camera focus appear to be blurry). 
                    
                    An advantage that the accumulation buffer has over the FSAA(and A-buffer) is that sampling does not have to be uniform orthogonal pattern within a pixel's grid cell. \emph{Rotate grid supersampling}(RGSS), this pattern gives more levels of anti-aliasing for nearly vertical and horizontal edges. 
                    
                    However, techniques like supersampling generating samples that are fully specified has relatively high cost. \emph{Multisampling} strategies lessen the high computational costs of these algorithms by sampling various types of data at different frequencies. It may have some samples per fragment on the edge of the object,  while may have one sample per fragment for shadow. Within GPU hardware, these techniques are called \emph{multisample anti-aliasing} and more recently, \emph{converge sampling anti-aliasing}, which saves time by computing fewer shading samples per fragment. If all MSAA positional samples are covered by the fragment, the shading sample is in the center. If the fragment covers fewer samples, the center may shift to avoid shade sampling off the edge of a texture. This is called \emph{centroid sampling} or \emph{centroid interpolation}.

                    MSAA is faster than a pure supersampling scheme because the fragment is shaded only once. It stores separate color and z-depth for each samples, which is not necessary. in CSAA, pixels are subdivided to subpixels which stores a index to the fragment it associated with. And a table with limited entries stores color and z-depth associated with fragment, which can be indexed by each subpixel  . It may occur artifacts with too many kinds of fragments within a pixel, however it's not so common in practice.

                    This idea is similar to \emph{A-buffer}, which is commonly used as software at non-interaction speed. For each polygon rendered, a \emph{converge mask} is created for coverage. The shade for the polygon associated with this coverage mask is typically computed once at the centroid location and shared with all samples on the fragment.The z-depth is also computed and stored in some way, even slope is retained for precise z-depth value. All the information will form a A-buffer fragment. 
                    
                    A critical way $A$-buffer is different from $Z$-buffer is that a screen grid cell can hold any number of fragments at one time, rather than maintaining one particular z-depth value. As they collect, fragments can be discarded if they are hidden judged from z-depth and coverage mask. Coverage mask can also be merged by \textbf{or} to form a larger area of coverage. Such merging can happen when a fragment buffer becomes filled, or as a final step before shading and display. After all the polygons are sent to A-buffer, colors of pixels can be computed by multiplying the percentage of coverage with fragments color. 

                    All these anti-aliasing technique result in better approximation of how each polygon covers a grid cell, however, they have limitations. One limitation is that a scene made of arbitrarily small objects. This can be solved by \emph{stochastic sampling}, which distribute samples randomly in the pixel with different sampling pattern at each pixel. The most common kind of stochastic sampling is \emph{jittering}, a form of \emph{stratified sampling} which works by place sample points in a random location of a subpixel divided equally. \emph{N-rooks sampling} is another form, for $n$ samples being placed in a  n$ \times $n grid. 
                    
                    Another technique called \emph{interleaved sampling} with different sample patterns can be intermingled in a repeating pattern and be done with a pure jittering scheme. It's seen as the generalization of accumulation buffer. 
                    
                    One real-time anti-aliasing scheme that lets samples affect more than one pixel is Quincunx method, also called \emph{high resolution anti-aliasing}. There four samples in the corner and the fifth in the center. In average every pixel has two samples, with the center has a weight of $\frac{1}{2}$ and the corner has a weight of $\frac{1}{8}$. This patter approximate a two-dimensional tent filter. Though this technique appears to die out, its method is reused, such as \emph{custom filter anti-aliasing} and FLIPQUAD which mix the idea of Quincunx and RGSS. 

                    Sampling rate can be varied, at low rate when scene is changing, and at high rate for static scene. 

        \subsection{Transparency, Alpha, and Compositing} 
            Transparency effect can be divided view based effect and light based effect. $Z$-buffer as the dominating algorithm, has a problem of not able to deal with a number of transparent objects overlapping on one pixel. One method for giving the illusion of transparency is called \emph{screen-door transparency}. This idea is to render the transparent polygon with a checkerboard fill pattern. That is, every other pixel of the polygon is rendered, thereby leaving the object behind it partially visible. However, a transparent object looks best when 50\% transparent; Only one transparent object can be convincingly rendered on one area of the screen. It's simple and the same idea is used in \emph{alpha to converge} at a subpixel level. 

            The concept of \emph{alpha blending} is used for blend transparent object's color with the color of the object behind it. Alpha value $\alpha$ is introduced describing the degree of opacity of an object fragment for a given pixel. To make an object transparent, it is rendered on the top of the existing scene with an alpha class less than 1.0. Every pixel will finally receive RGBA value to blend with its own color using \textbf{over} operator.
            $$\textbf{c}_o = \alpha_s\textbf{c}_s + (1 - \alpha_s)\textbf{c}_d$$
        
            $\textbf{c}_s$ and $\alpha_s$ refer to the incoming transparent object's color and alpha value(\emph{so-}\newline \noindent\emph{urce}).$\textbf{c}_d$ is the pixel color(\emph{destination}). Especially, when incoming object is opaque, which means $\alpha_s = 1$, it's the form of the original $Z$-buffer. It requires specific order, as the opaque objects been rendered first and the transparent object are blended on top of them in back-to-front order. The equation can be modeled for front-to-back-order, which is another blending mode called \emph{under operator}. However, sorting is not available some time. At this time, it's often best to use $Z$-buffer testing with all transparent objects at least appearing rather than $Z$-buffer replacement. Other technique also helps. 

            $A$-buffer can achieve hardware sorting rather than software sorting, and multisample fragment's alpha represents purely the transparency as it stores a separate converge mask. 

            Transparency can also be computed using two or more depth buffers and multiple passes: First, a rendering pass is made so that the opaque surfaces' $z$-depth are in the first $Z$-buffer. In the next pass, find the transparent surfaces that are closer than the first stored $Z$-buffer and the farthest among them to find the backmost transparent layer, then put their depths into second $Z$-buffer , and so on. The pixel shader can be used to compare $z$-depths in this fashion and so perform \emph{depth peeling}, where each visible layer is found in turn. 

            The \textbf{over} operator can also be used for anti-aliasing edges. Instead of storing a converge mask, an alpha can be generated to approximate the edge cover. This alpha value is then used to blend the object's edge with the scene, using \textbf{over} operator. It's unwise to generate alpha value for every polygon's edge. For example, when two adjacent polygons fully cover a pixel each by 50\%, when using the alpha it will lead to 75\% coverage over the pixel. This can be avoided by using converge mask or by blurring the edges outwards. In summary, alpha value can represent transparency and coverage. 
            
            The \textbf{over} operator turns out to be useful for blending together photographs or synthetic rendering of the objects. This process is called \emph{compositing}, leading to RGB$\alpha$ values. The $\alpha$ channel is sometimes called \emph{matte} and shows the silhouette shape of the object. 
            
            \emph{Additive blending} is another way: 
                $$\textbf{c}_o = \alpha\textbf{c}_s + \textbf{c}_d$$

            This blending mode doesn't require sorting between triangles. It works well for glowing effects that do not attenuate the pixel behind them but only brighten them. It's not suitable for transparency but work well for semitransparent surfaces. 

            The most common way to store synthetic RGB$\alpha$ images is with \emph{pre-multiplied alphas} (\emph{associated alpha}). That is, the RGB values are multiplied by $\alpha$ before stored(RGB values are smaller than $\alpha$), which make \textbf{over} operator more easily: 
                $$\textbf{c}_o = \textbf{c}_s^{'} + (1 - \alpha)\textbf{c}_d$$

            Another way images are stored is with \emph{un-multiplied alphas} (\emph{un-associated alpha}), means that the RGB value is not multiplied by $\alpha$. It's rarely used in synthetic image but has the advantage of represent the actual color. It's also useful to mask a photograph without affecting the underlying image's origin data. 

            A concept related to the alpha channel is \emph{chroma-keying}.It derives from the term \emph{green-screen} or \emph{blue-screen matting}. The idea here is that a particular color is designated to be transparent, where $\alpha$ needs to be stored. This allows images to be given an outline shape by using just RGB values with out $\alpha$. The drawback is that the actual $alpha$ of image is 0.0 or 1.0. 

            Though \textbf{over} operator can represent transparency, it's better to be the approximation of pixel coverage. In reality, the transparent object is represented of filtering and reflection. To filter, the scene behind the transparent object should be multiplied by objects spectral opacity. Reflection is to multiply the frame buffer by one RGB color and add another RGB color, which can be done in two process or \emph{dual-color blending}. 
            
        \subsection{Gama Correction}
            The content above discuss about the pixel values. For display, \emph{cathode-ray tube} (CRT) monitors exhibit a power law relationship between input voltage and display radiance, which turns out to match the inverse of light sensitivity of human eyes, leading to that an encoding proportional to CRT input voltage is roughly \emph{perceptually uniform}. This near-optimal distribution of values minimizes \emph{banding} artifacts. The desire for compatibility is another important factor. LCD has different tone response curve with hardware providing compatibility. 

            The \emph{transfer functions} that define the relationship between radiance and encoded pixel values are slightly modified power curves by the exponent $\gamma$:
                $$\textbf{f}_{\text{xfer}}(x) \approx x^{\gamma}$$
            
            Two $\gamma$ values are needed to fully characterize an imaging system. The \emph{encoding gamma} describes the \emph{encoding transfer function}, which is the relationship between scene radiance values captured by a imaging device and encoded pixel values. The \emph{display gamma} characterizes the \emph{display transfer function}, which is the relationship between encoded pixel values and displayed radiance. The product of the two gamma values is the overall or \emph{end-to-end gamma} of the \emph{end-to-end transform function}. It seems to be ideal to have end-to-end gamma being 1, there two differences: Absolute display radiance is much less than scene radiance; the \emph{surrounded effect}, refers to the fact that the the original scene radiance values fill the entire filed of view of the observer, while the display radiance values are limited to a screen surrounded by ambient room illumination. To counteract that, a non-unit end-to-end $gamma$ is used from 1.5 for dark environment to 1.125 for bright environment. 

            The relevant gamma for rendering purposes is encoding gamma. For television is 0.5, and for personal computer with a standard called \emph{sRGB} is 0.45. The sRGB standard assume the display gamma is 2.5 as the CRT usually has and set to 0.45 to ensure an appropriate end-to-end gamma.
            
            Most image data read to rendering system has been applied to encoded transfer function, which turns from linear-space to a non-linear space. \emph{Gamma correction} guarantee the correctness of linear interpolating between radiance. Ignoring gamma correction also affects the quality of anti-aliased edges like \emph{roping}.Fortunately, modern GPU can be set to automatically apply the encoding transfer function when values are written to the color buffer. But this feature can't be misused. It's important to apply conversion at the final stage(Where the values are written to the display buffer for the last time). This doesn't mean intermediate buffers can't contain nonlinear encodings, but it must be converted carefully before post-processing. 
            
            It's also necessary to convert any nonlinear input values to a linear space as well, such as texture. GPU is now available to convert it automatically. For authoring texture, care must be taken to use the correct color space. Various other inputs need similar conversion as well.
            \newpage

\section{Texture}
\subsection{The Texturing Pipeline}
    Texturing, at its simplest, is a technique for efficiently modeling the surface's properties. The pixels in the image texture are called \emph{texels}. The gloss texture modifies the gloss value, and \emph{bumping texture} changes the direction of the normal, which will all influence lighting equation.
    
    \begin{align*}
        \text{\emph{{object space location}}} 
        &\xrightarrow{projector\ function} \emph{parameter space coordinate}\\
        &\xrightarrow{corresponder\ function} \emph{texture space coordinate} \\
        &\xrightarrow{obtain\ value} \emph{texture value}\\
        &\xrightarrow{value transform function} \emph{transformed  texture value}
    \end{align*}

    This projector function is called \emph{mapping}, which needs to \emph{texture mapping}.

    \subsubsection{The Projector Function}
        Projector functions typically work by converting a three-dimensional point in space into texture coordinates, including spherical, cylindrical, planar projections. Problems may occur at the seams where faces meet. \emph{Polycube map} maps a model to a set of cube with different volumes of space mapping to different cubes. Other form of projector functions are not projections but are implicit part of surface formation. The goal of the projector function is to generate texture coordinate. 

        Various projector functions can be applied to a single model. most projector function are applied in modeling stage and results are stored in vertices. Some functions require vertex or pixel shader like animation and \emph{environmental mapping}.

        Spherical projector function, according to spherical coordinates: 
        $$\phi(x,\ y,\ z) = (\frac{\pi + \textbf{atan2}(y,\ x)}{2\pi}, \frac{\pi - \textbf{acos}(\frac{z}{\parallel x \parallel})}{\pi})$$

        Cylindrical:
        $$\phi(x,\ y,\ z) = (\frac{\pi + \textbf{atan2}(y,\ x)}{2\pi}, \frac{1}{2}(1 + z))$$
        
        Planar projection simply uses orthogonal projection to apply texture maps to characters, as the texture glued onto a paper doll. 
        \begin{equation*}
            \phi(x,\ y,\ z) = (\frac{\tilde{u}}{w}, \frac{\tilde{v}}{w}), \quad \text{where}
            \begin{pmatrix}
                \tilde{u} \\
                \tilde{v} \\
                * \\
                w 
            \end{pmatrix}
            = \textbf{P}_t
            \begin{pmatrix}
                x \\
                y \\
                z \\
                1
            \end{pmatrix}
        \end{equation*}


        Artists often must manually decompose the model into near-planar pieces to avoid distortion by unwrapping the mesh. The goal is to have each polygon be given a fairer share of a texture's area, while also maintaining mesh connectivity (without too many seams).
        
        The parameter space is not always a two-dimensional plane; sometimes is a three-dimension volume. For three-element vector $(u,\ v,\ w)$, $w$ represents the depth along the projection direction. For four-coordinate $(s, t, r, q)$, $q$ is used as fourth value in a homogeneous coordinate for spotlighting effect. Another important type of parameter space is directional, where each point in the parameter space represents a direction. The most common one is \emph{cube texture}. The one-dimensional projector function also have its own use(coloration according to altitude). Line can also be textured.(Rain is textured with a semi-transparent image)

    \subsubsection{The Corresponder Function}
        Corresponder function convert parameter-space coordinate to texture-space location in order to provide flexibility. One type is to use the API to select a portion of existing texture for display. Another type is a matrix transformation, which can be applied in vertex or pixel shader. The order of the transformation must be reserved as it's the location of image  will be changed rather the location of object. 

        Another class of corresponder functions controls the way an image is applied when parameters are out of range $[0, 1)$.

        \textbf{Warp}(DirectX), \textbf{Repeat}(OpenGL), or\textbf{title}: The image repeats itself across the surface; algorithmically, the integer part will drop. 
        
        \textbf{mirror}: The image repeats itself across the surface, but is mirrored on every other repetition. This provide some continuity along the 

        \textbf{Clamp}(DirectX), \textbf{Clamp to edge}(OpenGL): Values out of range of$[0, 1)$ will clamp to the edge. This function is useful for avoiding accidentally taking samples from the opposite edge of a texture when bilinear interpolation happens near a texture's edge. 

        \textbf{Border}(DirectX), \textbf{Clamp to Border}: Values out of range of $[0, 1)$ will be set to a border color. This function is good for rendering decal onto surfaces. 

        The \emph{periodicity} problem like repeating tiles of a texture can be solved by combining the texture values with another, non-tiled texture. Another option to avoid periodicity is to use shader programs to implement specialized corresponder functions that randomly recombine texture pattern or tiles. \emph{Wang tiles} are one example which choose randomly from small square tiles with matching edges. 

        For real-time work, the last corresponder function is implicit, and is derived from the image's size. It allows the parameter being in range  $[0, 1)$ with different resolution of texture can be applied. 
\subsubsection{Texture Value}
    The texture value is retrieved after corresponder function. For image textures, this is done by using the texture to retrieve texel information from the image. Procedural functions are sometimes used. 
    
    The most common texture values include RGB triplet, gray scale and RBG$\alpha$ value. The values returned from the texture are optionally transformed before use. One common example is the remapping of data from unsigned range to a signed range. Another one is to compare the value with a reference value and return a flag.
    
\subsection{Image Texturing}
The texture used in GPU has usually $2^m \times 2^n$ texels. Modern GPU allows for an arbitrary size. When finally projecting texture onto the screen, the projected square may contain \emph{magnification} and \emph{minification}. The filtering can take place in the input(values read from  textures) or in the output(final pixel colors). When input and output is linear, there will be no difference. 

\subsubsection{Magnification}
    The most common filtering techniques for magnification is \emph{nearest neighbor}(application of box filter) and \emph{bilinear interpolation}. There is also \emph{cubic convolution}, enabling high quality. It's not commonly available in the native hardware but can be realized in shader program. 
    
    One characteristic of nearest neighbor is that the individual texels may become apparent, which is called \emph{pixelation} for every pixel only fetch one nearest texel to each pixel center. 

    In bilinear interpolation, for each pixel, this kind of filtering finds the four neighboring texels and linearly interpolates in two dimension to form a blended value for pixel. The cubic convolution is more expensive.  Specifically, for a image coordinate $(p_u, p_v)$. According to the coordinate, four centers of texels can be calculated.

    $$x_l = \lfloor p_u \rfloor, \quad y_b = \lfloor p_v \rfloor$$
    $$x_r = \lceil p_u \rceil, \quad y_t = \lceil  p_v \rceil$$

    When interpolating:

    $$u^{'} = p_u - \lfloor p_u \rfloor,\quad v^{'} = p_v - \lfloor p_v \rfloor$$
    \begin{align*}
        \textbf{c}(p_v,\ p_u) &= (1 - u^{'})(1 - v^{'})\textbf{t}(x_l,\ y_b) \\
            &+ u^{'}(1 - v^{'})\textbf{t}(x_r,\ y_b) \\
            &+ (1 - u^{'})v^{'}\textbf{t}(x_l,\ y_t) \\
            &+ u^{'}v^{'}\textbf{t}(x_r,\ y_t)
    \end{align*}

    This kind of interpolation will have poor performance when the image has a nonlinear change such as the edge of a object. 

    A common solution to the blurriness that accomplish magnification is to use \emph{detail textures}. These are textures that represent fine surface details. The high-frequency of repetitive pattern of the detail texture, combined with low-frequency magnified texture will have a similar visual effect with high resolution texture. 
    
    Sometime, linear interpolation is not required. In order to magnify checkerboard, remapping will have better visual effect. Such remapping is useful in cases where well-defined edges are important, such as text. Alpha value can be used to be threshold the results. This method is called \emph{cutout texture}. \emph{Vector textures} can handle high quality edges generation. It stores information at texel describing how the edges cross the grid. These information can contain more subjects leading to accuracy and resolution-independence. However, it's too costly to apply to read-time rendering application. 
    
    There is simpler technique using \emph{sampled distance filed} data structure. A distance filed is a scalar signed field over a space or surface in which an object is embedded(that means, the filed can be curved). At any point, the distance field has a value with a magnitude equal to the distance to the object's nearest boundary point. The value is negative within the object and positive outside the object. This will interpolated with the real distance rather than space coordinate so it's linear and fit bilinear interpolation to get a sampled distance filed for further rendering. The evaluation of sampled distance field is to use alpha map and built-in GPU interpolation and alpha testing rather than pixel shader modification.   
    
\subsubsection{Minification}
    Directly, we consider how to aggregate the influence of texels, which is hard to accomplish.

    Nearest neighbor can be used to use one texel at the very center of the pixel to represent, which will cause big alias and even noticeable when surface is moving and result  \emph{temporal aliasing}. Similarly, bilinear interpolation can be used but don't perform much better than nearest neighbor. 

    According to Nyquist, we need to increase the pixel sample rate or the the texture frequency has to decrease. The previous chapter introduces increasing of sampling rate, but has a limited increase that don't fit minification. The design of anti-aliasing algorithm is to pre-process texture and create data structure to help with effect approximation.
    
    The most popular method is called \emph{mipmapping}. Mipmapping generate a set of images, called \emph{mipmap chain}, with a level start at 0 and each level downsample to a quarter of the original area as the next level. level 0 is the original one and the downsampled ones are called subtexture. 

    Two important elements in forming high-quality mipmaps are good filtering and gamma correction. The common way to form a mipmap level is to take each $2 \times 2$ sets of texels and average them to get the mip level texel. This is actually a 2-dimensional box filter, which is better to use a Gaussian filter instead. 

    For textures encoded in a non-linear space (such as color texture), gamma correction is obviously needed to apply to the texture before mipmapping to a linear space. After filtering, converting texture back to non-linear space is also needed. 
    
    For the texture is fundamentally in a non-linear space, specialized mipmapping method is required. 

    In order to measure the coverage of a single pixel, \emph{d} or $\lambda$ is introduced. There two approaches to compute it: First we use the longest edge of quadrilateral formed by the pixel's cell to approximate; Or we can select the largest absolute value of $\frac{\partial u}{\partial x}$, $\frac{\partial v}{\partial x}$, $\frac{\partial u}{\partial y}$, $\frac{\partial v}{\partial y}$. Each differentials is a measure of the amount of change in the texture coordinate with respect to a screen axis. These gradient value is not accessible by dynamic flow control. And as the vertex shader can't access derivative data so it needs to be calculated first and then sent into GPU. 

    \emph{d} is used to determine where to sample along the mipmap's pyramid axis. The goal of texel ratio is at least 1:1 according to Nyquist. The important principle is that the more texels a pixel covers and \emph{d} increases, a smaller and blurrier version of texture will be accessed by a triplet $(u, v, d)$. Instead of a integer representing mipmap level, $d$ is a fractional value between two level. Using u, v coordinates we can get two bilinear texture values by bilinear interpolation and linear interpolated them using the decimal part of \emph{d} to get the final value. This process is called trilinear interpolation and is performed in each pixel. 

    \emph{Level of detail bias(LOD bias)} is used to control by being added to \emph{d} in different circumstances. It can be specified for the texture as whole, or per-pixel in the pixel shader. 

    Instead of summing all the texels that affect a pixel, pre-combined set of texels are accessed and interpolated. It may lead to \emph{overblurring}, which happens when camera looks along nearly edge-on. It may have a rectangular area. In order to avoid aliasing, we may use the largest derivative as \emph{d} and result in relatively blurring. One extension to solve this is \emph{ripmap}, which will also create rectangular texture. However, it's too costly to use in real-time rendering. 

    Another method to avoid overblurring is \emph{summed-area table}(SAT). Every texel has more bits of precision and record its individual data(color for example) and the sum of all the corresponding texture's texels in the rectangle formed by this location and texel (0, 0). When a pixel grid is back-projected (which means project the pixel grid to the texture space; normally, we perceive as the texture projected on the pixel space on the screen) onto the texture, the projected gird is bounded by a rectangular. The average data in the rectangular will be returned as the texture value. 
    $$\textbf{c} = \frac{\textbf{s}(x_{ur}, y_{ur}) - \textbf{s}(x_{ll}, y_{ur}) - \textbf{s}(x_{ur}, y_{ll}) + \textbf{s}(x_{ll}, y_{ll})}{(x_{ur} - x_{ll})(y_{ur} - y_{ll})}$$

    However, both ripmap and summed-area table will have blurring image when a texture is viewed along the diagonal as the average of the whole texture will be averaged rather than the average the thin real projected rectangle. 

    Ripmap and  summed-area table are examples of \emph{anisotropic filtering} algorithm. They have good effect in horizontal and vertical directions but is memory intensive.  Summed-area table are more suitable in modern GPU with more algorithms to optimize. 

    Other from anisotropic filtering, \emph{unconstrained anisotropic filtering} is widely used. Similar to summed-area table, pixel grid is back-projected to a quad and sampled a number of time with associated squarish area. Instead of using a single mipmap sample to approximate the coverage, the algorithm uses a number of squares to cover the quad. The shorter side of the quad is used to determine \emph{d} with a relatively small averaged are for each sample. The longer side creates a \emph{line of anisotropy} parallel to the longer side and through the middle of the quad. Samples will be taken on the line and its amount is decided by the value of anisotropy. 
    
    This scheme allows the line of anisotropic to run in any direction (Correctly sample the diagonal view of texture as the line can be diagonal rather than horizontal and vertical in common anisotropic filtering). Meanwhile it has same cost as the mipmap does. 
\subsubsection{Volume Textures}
    A direct extension of image textures is three-dimensional image data that is accessed by $(u, v, w)$ value. Being inside A single mipmap level requires a trilinear interpolation, while filtering between mipmaps require \emph{quadrilinear interpolation}. This requires for high precision texture. Volume textures have the advantage that the good projection function which avoids distortion and seam. 
    
\subsubsection{Cube Maps}
    Another type of texture is the \emph{cube texture} and \emph{cube map}. The mapping point is specified by a three-dimensional vector. It points from the center of the cube and choose the texture face by the face with largest magnitude. (choose $-Z$ for $(1, 2, -3)$. The other entries will be divided by magnitude to be in range in [-1, 1] and can simply be mapped to range [0, 1]. It's useful for environmental mapping.

    Cube maps support bilinear interpolation and mipmaps, but may have problems near the seam as the sample can't be across the boundaries of faces and faces can't affect each other. Another problem that the angular size of a texel varies over a cube face.  That is, a texel at the center of a cube represents a greater visual solid angle than a texel at the corner. 

\subsubsection{Texture Caching}
     Memories for textures are always not enough. \emph{Texture Caching} aims to a balance between the overhead of uploading textures to memory and the amount of of memory taken up by textures at one time. 

     Some general advice is to keep the texture no larger than necessary and to try to keep polygons grouped by their use of texture. 

     A \emph{least recently used}(LRU) strategy is one commonly used in texture caching scheme. Every texture has a time stamp representing when it's last accessed. When new texture is uploaded, the texture with oldest time stamp is unloaded first. Priority is optionally given to prevent unnecessary texture swapping. 

     It's suggested to check the texture being swapped out for problems will happen when this texture is used in current frame. When it occurs, LRU has terrible performance. It better to first transfer to \emph{most recently used} and after no textures are swapped out to switch back to LRU. 

     \emph{Prefetching} where future needs are anticipated making the texture loading over a few frames as there may be sudden big amount of texture loading and it takes a lot of time. 

     When data set is huge, such as flight simulations program. The traditional approach is to break these images into smaller tiles that hardware can handle. A improved structure is \emph{clipmap}. The entire data set is treated as a mipmap, but only a small part of the lower levels of the mipmap is required for certain view and memory. 

\subsubsection{Texture Compression}
    \emph{Texture Compression} can directly attack memory and bandwidth problem. \emph{S3 texture compression} becomes the standard for DirectX and called \emph{DXTC} or \emph{BC}. It has independently encoded pieces on $4 \times 4$ texel blocks, which is also called \emph{tile}. Encoding is based on interpolation which is simple and fast. There are five variants: 
    
    \quad \textbf{DXT1 / BC1}: It has two 16-bit reference RGB565 values as the limitation. Each texel has a 2-bit interpolation factor to refer to 2 references or 2 intermediate value, while only 1 intermediate value is included and another stands for transparent when alpha is introduced. The compression ratio is 6 : 1 compressing 24-bit RGB texture. 

    \quad \textbf{DXT3 / BC2}: The encoding pattern is same as DXT1. Meanwhile, every texel has a 4-bit alpha value stored separately. The compression ratio is 4 : 1. 

    \quad \textbf{DXT5 / BC3}: The encoding pattern is same as DXT1. In addition, alpha data is encoded using two 8-bit reference values. Each texel has 3 interpolation factor which can refer to one of the reference alpha values or one of the six intermediate alpha values. 

    \quad \textbf{ATI1/ BC4}:  It supports single color channel and encoded as the alpha channel in DXT5.

    \quad \textbf{ATI2/ BC5}: It supports dual color channels and encoded as the aloha channel in DXT5. 

    The disadvantage of these compressions is \emph{lossy}. Once a tile has many distinct value it will lead to some loss. Fortunately, These compression scheme generally give acceptable image fidelity. Another problem is that all the colors are on a straight line in RGB space. \emph{color distribution} can attack it as it uses colors from the neighbors blocks in order to achieve more colors. 

    For OpenGL ES, \emph{Ericsson texture compression(ETC)} is chosen. It shares the same features as S3TC. It encodes $4 \times 4$ texels with 4 bits per texel. Each $2 \times 4$ or $4 \times 2$ block stores a basic color, and each texel in a block can select to add one of the values in the selected lookup table. 

    Compression of normal maps requires some care. We assume the normal is unit vector and the $z$-component is positive. So we can derive $z$ by: 
    $$n_z = \sqrt{1 - n_x^2 - n_y^2}$$
    
    So the three-dimensional texture can be stored as two-dimensional texture, a modest compression. Further compression is usually achieved in BC5 manner: A block of texel has 16 possible values, which can be bounded by a bounding box. So x, y values can be perceived as two channels of color and interpolation factor allows for value to choose from for each axis. 

    A fallback when GPU does not support BC5 can use DXT5 to substitute. 

    For normal maps, the aspect ratio decides the layout of the normal inside a block. For example, as the normal has a width nearly twice as height, the current $8 \times 8$ grids can be converted to $4 \times 16$ grids with different bits allocation. 

\subsection{Procedural Texturing}
\emph{Procedural texture} is to evaluate a function to look up texture value rather than generate texture space coordinate. It's often used in offline rendering system and not so common in real-time rendering. Procedural texture is used when image texture requires for costly memory access. Volume texture is a typical example. 

\emph{Cellular texture}, calculated the closest distance to a set a special points for every location. The color or shading normal will change by this distance. 

Another type of application is on physical simulation like water ripples. 

As parameterization will be more difficult for procedural texture, it's better to synthesis texture onto the surface directly. Anti-aliasing procedural texture becomes both easier and more difficult. On one hand, pre-computations methods are not available; On other hand, procedural texture have more inside information. 

\subsection{Texture Animation}  
The image applied and coordinate can both be dynamic. For example, dynamic coordinate for water downfall to make the water move. The texture can be applied to matrix transformation and blending techniques. 

\subsection{Material Mapping}
A common use of a texture is to modify a material property affecting the shading equation. Shader can read values from the texture directly. For equation:
$$\textbf{L}_o(\textbf{v}) = (\frac{\textbf{c}_{\rm{diff}}}{\pi} + \frac{m + 8}{8\pi}\bar{\cos^m{\theta_h}}\textbf{c}_{\rm{spec}}) \otimes E_L\bar{\cos{\theta_i}}$$

$\textbf{c}_{\rm{diff}}$, $\textbf{c}_{\rm{spec}}$ and $m$ is the material information can be read from textures, particularly \emph{diffuse color map, specular color map}(grayscale value rather than RGB for the reality) and \emph{gloss map}(describing the smoothness). As $\textbf{c}_{\rm{diff}}$, $\textbf{c}_{\rm{spec}}$ is linear to the output, the input can be filtered without anti-aliasing, when $m$ needs some care. 
\subsection{Alpha Mapping}
The alpha map can be used for many interesting effect like decaling with clamp corresponder function and transparent edge to present the part you want. A similar application is in making cutouts that have 3D visual effect like cross tree. In order to reserve the visual effect from other angle, cross tree is set by rotating the cutouts. As illusion will break down when viewer see from the above, more cutouts can be added. Combining alpha texture and texture animations can make many visual effects. 

Alpha map has many options like alpha blending. As discussed before, alpha blending needs a relative order. \emph{Alpha test} is another option that conditionally discarding pixels with alpha values below a given threshold in the merge unit or pixel shader, which enable polygons to be rendered in any order. 

\emph{Alpha to coverage}, and the similar feature \emph{transparency adaptive anti-aliasing}, take the transparency value of the fragment and convert this into how many samples inside a pixel are covered. This is similar to screen-door transparency but at a sub-pixel level. The alpha value determines the proportion of samples being covered within a pixel. 

\subsection{Bump Mapping}
\emph{Bump mapping} is used to represent small-scale detail which is implemented in per-pixel shading. The scale of detail on object can be classified into: \emph{macro-features}, \emph{meso-feature} and \emph{micro-features}

Macro features cover many pixel. Macro-geometry is represented by geometric primitives, while micro-geometry is encapsulated in the shading model and used in pixel shader using texture maps as parameters for equation. Meso-geometry describe everything between these two scales.

\emph{Bumping mapping techniques} are commonly used for mesoscale shading, adjusting the shading parameters(like normals) at pixel level in such a way that the viewer perceives small perturbations away from the base geometry. For example, when introducing a change to color component, instead of changing the base texture directly, we access another texture which is used to modify the surface normal. This will preserve the original geometry with different effect. 

For bump mapping, the normal must change direction with respect to some frame of reference. \emph{Tangent space basis} is stored per vertex used to transform the lights to a surface location's space to compute the effect of perturbing the normal and is expressed by \emph{tangent and bitangent vectors} and corresponding basis matrix.
\begin{equation*}
    \begin{pmatrix}
    t_x & t_y & t_z & 0 \\
    b_x & b_y & b_z & 0 \\
    n_x & n_y & n_z & 0 \\
    0 & 0 & 0 & 1
    \end{pmatrix}
\end{equation*}  

Tangent and Bitangent don't have to truly be perpendicular to each other since the normal map itself maybe distorted to fit the surface. The normal can be calculated by the product of tangent and bitangent vectors, which can save memory with attention to handedness(e.g, in symmetric model). The handedness can be marked with a bit. The idea of tangent space is important for other algorithms like the orientation of the material. 

    \subsubsection{Blinn's Methods}
        The original bump mapping method stores two offset values $b_u$ and $b_v$ for interpolated normals along \textbf{u} and \textbf{v} image axes. This type of of bump map texture is called an \emph{offset vector bump map} or \emph{offset map}.

        Another way to represent bumps is to use \emph{heightfield} to represent bumps so the direction of normals. 
        
    \subsubsection{Normal Mappings}
        \emph{Normal map} directly stores perturbed normals, represent the same effect with original bump mapping with different storage format. $(x,y,z)$ values are mapped to [-1, 1], e.g., for an 8-bit texture the $x$-axis value 0 represents -1.0 abd 255 represents 1.0. It's similar to RGB values and light blue [128, 128, 255] represents flat surface. 

        Originally, the normal map is in world-space. However, this approach can't deal with any deformation, like rotation, which will lead to change in orientation. So it's rarely used. 

        Normal map can also be defined in object space. It now can deal with rigid transformation. Lights should be transformed into object space which can be done in application stage. 

        Tangent space is usually used. It allows for deformation of the surface and maximal usage of normal maps. It can be compressed as introduced before while requiring or more transformations. 

        There are two methods to use normal maps. If there is seldom lights, it's preferred to transform the light to tangent space as it move slowly per pixel and can be interpolated in triangle. However, it's preferred to transform perturbed normals to world space when there are more lights as it involves less transformation and avoid tangent space distortion. 

        Filtering normal maps is a difficult problem as the relationship between normal and shaded color is not linear. However, Lambertian surfaces are a special case where the normal map has an almost linear effect on shading. In Lambertian shading, it satisfies: 
        \begin{equation*}
            \textbf{l} \cdot (\frac{\sum_{j = 1}^n\textbf{n}_j}{n}) = \frac{\sum_{j = 1}^n(\textbf{l} \cdot \textbf{n}_j)}{n} 
        \end{equation*}

        So Lambertian shading \emph{almost} produce the right result while it's a \emph{clamped} dot product. But in practice it's not objectionable. 

        For other surfaces, more details are included in following chapters. 

    \subsubsection{Parallax Mapping}
        A problem with normal mapping is that the bumps never block each other. The idea of \emph{parallax mapping} is an approximation to shift the pixel location to simulate this effect. 

        The amount to shift is based on the height retrieved and the angle of the eye to the surface.The heightfield values are scaled and biased before being used. The scale determines how height the heightfield meant to extend and the bias gives a sea-level height at which no shift take place. The equation follows: 
        \begin{equation*}
            \textbf{p}_{\text{adj}} = \textbf{p} + \frac{h\cdot \textbf{v}_{xy}}{v_{z}}
        \end{equation*} 
        
        Note that unlike most shading equations, here the view vector needs to be in tangent space. The new location also uses the same height as it's about the same. However, this method falls apart at shallow viewing angles. In order to solve this, limited the amount of shifting is needed: 
        \begin{equation*}
            \textbf{p}_{\text{adj}}^{'} = \textbf{p} + h\cdot \textbf{v}_{xy}
        \end{equation*}

        It has some drawbacks, like lessen bumpiness at shallow angles, etc. But it is still considered the practical standard for bump mapping. 

    
    \subsubsection{Relief Mapping}
        Parallax mapping is just an approximation. The actual effect we want is what is visible at the pixel, or where the view vector first  intersects the heightfield. 

        Methods are similar to traditional ray tracing with different implementations. Relief mapping is actually a class of three independent research outcomes based on same approaches. 

        The key idea is to test a fixed number of texture samples along the projected vector in the cut of the surface along the view direction. The algorithm finds the first intersection of the eye ray with the line segments approximating the curved height field. Once the location is determined, the attached map will be used. It can also be used to have the  bumpy surface casts shadows onto it self.  

        The problem of determining the actual intersection point is a root-finding problem. There is a critical importance in sampling the heightfield frequently enough. This can be done either by using better filtering methods or higher resolution of heightfield texture. 

        Another approach to increasing both performance and sampling accuracy is to not initially sample the heightfield at a regular interval, but instead to try to skip intervening empty space. 

        One problem with relief mapping methods is that the illusion breaks down along the silhouette edges of objects.  The key idea is that the triangles rendered define which pixels should be evaluate by the pixel shader program, not where the surface actually is located. \emph{Shell map} is introduced to extrude each polygon in the mesh outwards and form a prism. Rendering this prism forces evaluation of all pixels in which the heightfield could possibly appear. 
    \subsubsection{Heightfield Texturing}
        \emph{Displacement mapping} method has a flat meshed polygon access a heightfield texture and the height retrieved from the texture is used by the vertex shading program to modify the vertex's location. So the heightfield is called \emph{displacement texture}. It's a similar concept to relief mapping. They both have a drawbacks like making collision detection more challenging. 

\newpage

\section{Advanced Shading}
    \subsection{Radiometry}
        Radiometry deals with the measurement of electromagnetic radiation consisting of a flow of \emph{photons} which behave as either particles or waves. One wave-related property of photons which can't be disregarded is the fact that each has an associated frequency or wavelength, and the energy of each photon and interaction with rods and cones are to do with its frequency.  Different frequencies of photons are perceived as light of different colors or not perceived at all. The relation between wavelength $\lambda$, frequency $\nu$ and energy $Q$: 

        $$\nu = \frac{c}{\lambda}$$
        $$\lambda = \frac{c}{\nu}$$
        $$Q = h\nu$$

        where $c$ is the speed of light and $h$ is Planck's constant. 

        Electromagnetic radiation exists from ELF(extremely low frequency) radio waves to gamma rays.  \emph{visible spectrum} that is from roughly 380 to 780 nanometers. Colors are perceived for \emph{monochromatic} light. The various radiometric units are presented below. 
        \begin{center}
            Wavelength:\quad m

            Frequency:\quad Hertz
            
            Radiant Energy:\quad J 
            
            Radiant flux:\quad W

            Irradiance:\quad W/$\text{m}^2$

            Radiant intensity:\quad W/$\text{sr}$

            Radiance:\quad W/$(\text{m}^2\text{sr})$
        \end{center}
    
        The \emph{radiant flux} or \emph{radiant power}, $\Phi$ or $P$, of a light source is equal to the number of joules per second emitted.
        $$\Phi = \frac{dQ}{dt}$$

        \emph{Irradiance} is the density of radiant flux with respect to area.
        $$E = \frac{d\Phi}{dA}$$

        irradiance $E$ is used to measure light flowing into a surface and exitance $M$ (also called \emph{radiosity} or \emph{radiant exitance} is used to measure light flowing out of a surface, and both of them can be referred by \emph{radiant flux density}. The irradiance is constant for a single and further directional light. Assume a small light bulb in relatively large space so the light can be considered to be emitted from a point. When we want to examine the irradiance in a particular direction using a narrow cone, we can find: 

        $$E_L(r) \propto \frac{1}{r^2}$$

        r is the distance from the light source to the surface. 

        $$E_L(r) = \frac{I}{r^2} \quad \Rightarrow \quad I = E_L(r)r^2$$

        This quantity, $I$, is called \emph{intensity} or \emph{radiant intensity}. We observe when $r = 1$, intensity is flux density with respect to an area on an enclosing unit sphere, similar to the definition of radians. A \emph{solid angle} is a three-dimensional extension of the concept,  measured with \emph{steradians}(sr). Using the definition of solid angle:
        $$I = \frac{d\Phi}{d\omega}$$

        When light sources can be treated  as a point, it's called \emph{point lights}. Intensity is useful for measuring the illumination of a point light source. If the distance from the light source is five times or more that of the light's width, the light source can be recognized as point light when considering the relationship between intensity  and irradiance.
        
        As mentioned before, \emph{radiance} $L$ measures density of light flow per unit area for a given ray direction, which is important  for rendering. 
        $$L = \frac{d^2\Phi}{dA_{\text{proj}}d\omega}$$

        where $dA_{\text{proj}}$ refers to $dA$ projected to the plane perpendicular to the ray. We can also perceive the projection as there is a mixed directional light perpendicular to a small area. The relationship between them is : 

        $$dA_{\text{proj}} = dA\overline{\text{cos}}\theta$$

        Where $\theta$ is the angle between two surface normals. 

        It's helpful to perceive: 
        $$L = \frac{dI}{dA_{\text{proj}}} = \frac{dE_{\text{proj}}}{d\omega}$$

        The radiance in an environment can be thought of as a function of five variables (or six, including wavelength), called the \emph{radiance distribution}. Three of the variables specify a location, the other two a direction. This function can describes all light traveling anywhere in space.

        An important property of radiance is that it is not affected by distance. To perceive this, what is happening physically is that the solid angle covered by the light's emitting surface gets smaller as distance increases. For the light bulb example: 
        $$L^{'} = \frac{dE_{\text{proj}}\cdot\frac{k}{r_1^2}}{d\omega\cdot\frac{k}{r_1^2}} = L$$

    \subsection{Photometry}
        \emph{Photometry} is like radiometry, except that ti weights everything by the sensitivity of the human eye. The results of radiometric computations are converted to photometric units by multiplying by the \emph{CIE photometric curve}, a bell- shaped curve centered around 555nm that represents the eye's response to various lengths of light. The another difference is the unit. 

        Photometry does not deal with the perception of color itself, but rather with the perception of brightness from light of various wavelengths. The units for photometry: 
        
        \begin{center}
            Luminous Energy:\quad talbot

            Luminous Flux:\quad lumen(lm) 

            Illuminance:\quad lux(lx) 

            Luminous intensity:\quad candela(cd) 

            Luminance: \quad cd/$\text{m}^2$ = nit
        \end{center}

        Luminance is often used to describe the brightness of flat surfaces. 

    \subsection{Colorimetry}
        Light from a given direction consists of a set of photons in some distribution of wavelengths. This distribution is called the light's \emph{spectrum}. Three numbers can be used to precisely represent any spectrum seen as only three different signal can be receives. 

        By color matching experiments, as color can be divided two three base color in [-1, 1] knob range or energy value, three values are derived for each wavelength and for every wavelength it forms a color matching curve. The relative amount of three values decide the visual color. Given an arbitrary spectrum, multiply it with three color matching curve and the integral of the resulting curves give the relative amount of three values. So different spectra can resolve to the same three weights and representing same color perceived. Matching spectra presenting same color are called metamers. 

        $r$, $g$, $b$ value can not represent all color directly for negative weights. $\overline{x}(\lambda )$, $\overline{y}(\lambda)$ and $\overline{z}(\lambda)$ is used instead as no negative weight appears. $\overline{y}(\lambda)$ color is the same one as the photometric curve. The surface reflectance and light source will define a color function $C(\lambda)$. Like $r$, $g$, $b$ weights: 
        
        $$X = \int_{380}^{780}C(\lambda)\overline{x}(\lambda)d\lambda$$
        $$Y = \int_{380}^{780}C(\lambda)\overline{y}(\lambda)d\lambda$$
        $$Z = \int_{380}^{780}C(\lambda)\overline{z}(\lambda)d\lambda$$

        These $X$, $Y$, and $Z$ \emph{tristimulus values} are weights that define a color in CIE XYZ space. A better three-dimensional space is use the plane $X + Y +Z = 1$ as the relative weights count. 

        $$x = \frac{X}{X + Y + Z}$$
        $$y = \frac{Y}{X + Y + Z}$$
        $$z = \frac{Z}{X + Y + Z}$$

        As $z$ value is omitted, the plot of the \emph{chromaticity coordinates} $x$ and $y$ values is known as the \emph{CIE chromaticity diagram}. The curved line in the diagram shows where the spectrum lie, and the straight line connecting the ends of the spectrum is called the \emph{purple line}. For $x = y = z = \frac{1}{3}$, it's used to define white. For a computer monitor, the \emph{white point} is the combination of the three color phosphors at full intensity. 

        Given a color point $(x, y)$, draw a line from the white point through this point to the spectral line. The relative distance of the color point compared to the distance to the edge of the region is the \emph{saturation} of the color. The point on the region edge defines the \emph{hue} of the color.

        The third dimension to fully describe a color is the $Y$ (the curve is the same as photometric curve) value, luminance, defining a $xyY$ coordinate system. The display system is limited in its ability to present different colors by the spectra of its three channels. An obvious limitation is in luminance and saturation.

        The triangle in the chromaticity diagram represents the \emph{gamut} of a typical computer monitor. The three corners of the triangle are the most saturated red, green and blue colors. An important property of the chromaticity diagram is that these limiting colors can be joined by straight lines to show the limits of the monitor as a whole. 

        Conversion from $XYZ$ to $RGB$ space is linear: 

        \begin{equation*}
            \begin{pmatrix}
                R \\
                G \\
                B
            \end{pmatrix}
            =
            \begin{pmatrix}
                3.24049 & -1.537150 & -0.498535 \\
                -0.969256 & 1.875992 & 0.041556 \\
                0.055648 & -0.204043 & 1.057311 
            \end{pmatrix}
            \begin{pmatrix}
                X \\
                Y \\
                Z
            \end{pmatrix}
        \end{equation*}
        
        This conversion matrix is for a monitor with a D65 white point. Some XYZ values can transform to RGB values that are negative or greater than one. Theses are colors out of gamut. A common conversion is to transform a RGB color to a grayscale luminance value is use the inverse of the matrix: 

        $$Y = 0.212671R + 0.715160G + 0.072169B$$

        While any given spectrum can be precisely represented by an RGB triplet, there is some contrast. For example, multiplying two RGB colors is not the sam as multiplying two spectra. However, in practice ,multiplying RGB values together works well.
    
    \subsection{Light Source Types}
        The combination of light source and housing is called a \emph{luminaire}. In rendering it's common to model the luminaire as a light source with a directional distribution that implicitly models the effect of the housing. 

        Directional lights are the simplest model. They are fully described by \textbf{l} and $E_l$. Recall $E_L$ is expressed as an RGB vector for rendering purposes. 
        
        Point lights have a softer assumption compared to directional light that can be applied to more circumstances. 

        For all variants of directional and point lights introduce in this section, they follow the same assumption: at a give surface location, each light source illuminates the surface from one direction only. 
        
        \subsubsection{Omni Lights}
            Point lights with a constant value for $I_L$ are known as \emph{omni lights}. $I_L$ is also expressed as an RGB vector. For computations: 
            $$r = \| \textbf{p}_L - \textbf{p}_S \|$$
            $$\textbf{l} = \frac{\textbf{p}_L - \textbf{p}_S}{r}$$
            $$E_L = \frac{I_L}{r^2}$$

            It's often  preferable to use \emph{distance falloff functions} instead to describe the relationship between $E_L$ and distance.
            $$E_L = I_Lf_{\text{dist}}(r)$$

            Distance falloff functions can proved ore control for lighting. One historically important falloff function: 

            $$f_{\text{dist}}(r) = \frac{1}{s_c + s_lr + s_qr^2}$$

            where $s_c$, $s_l$ and $s_q$ are properties of light source. 

            A much simpler one used in games and modeling: 

            $$
            f_{\text{dist}}(r)=
                \begin{cases}
                    1 & \text{where } r \leqslant r_{\text{start}}\\
                    \frac{r_{\text{end}} - r}{r_{\text{end}} - r_{\text{start}}} & \text{where } r_{\text{start}} < r < r_{\text{end}}\\
                    0 & \text{where } r \geqslant r_{\text{end}} 
                \end{cases}
            $$

        \subsubsection{Spotlights}
            In reality, different visual effects can be produced using different functions to describe how $I_L$ varies with direction. One important type of effect is \emph{spotlight}. The angle $\theta_s$ is defined as the angle between the spotlight direction \textbf{s} and $-\textbf{l}$. In OpenGL: 

            $$
            I_L(\textbf{l}) = \begin{cases}
                I_{L_{\text{max}}}(\cos{\theta_s)}^{s_{\text{exp}}} & \text{where } \theta_s \leqslant \theta_u \\
                0 & \text{where } \theta_s > \theta_u
            \end{cases} 
            $$

            $s_{\text{exp}}$ controls the tightness and $\theta_u$ controls the edge as the \emph{umbra angle}.
        
            In DirectX: 

            $$
            I_L{\textbf{l}} = \begin{cases}
                I_{L_{\text{max}}} & \text{where } \cos{\theta_s} \geqslant \cos{\theta_p} \\
                I_{L_{\text{max}}} (\frac{\cos{\theta_s} - \cos{\theta_u}}{\cos{\theta_p} - \cos{\theta_u}})^{s_{\text{exp}}} & \text{where } \cos{\theta_u} < \cos{\theta_s} < \cos{\theta_p} \\
                0 & \text{where } \cos{\theta_s} \leqslant \cos{\theta_u}
            \end{cases}
            $$

            The angle $\theta_p$ defines the \emph{penumbra angle} of the spotlight, or the angle at which its intensity starts to decrease. 
        \subsubsection{Textured Lights}
            Textures can be used to add  visual richness to light sources and allow for complex intensity distribution or spotlight functions. Projected textures can be sued for projector effects limited in a frustum. These lights are often called \emph{gobo} or \emph{cookie} lights.  
            
            For lights that are not limited to a frustum but illuminate in all directions, a cube map can be used. Textures can be added to any light type to enable additional visual effects. Textured lights allow for easy control of the illumination by artists. 
    \subsection{BRDF Theory}
        \subsubsection{The BRDF}
            In radiometry, the function that is used to describe how a surface reflects light is called the \emph{bidirectional reflectance distribution function}(BRDF).  The precise definition of the BRDF is the ratio between differential outgoing radiance and differential irradiance. 

            $$f(\text{l}, \text{v}) = \frac{dL_o(\textbf{v})}{dE(\textbf{l})}$$

            The value of the BRDF depends on wavelength, so for rendering purposes it is represented as an RGB vector. 

            For non-area light sources, the BRDF definition can be expressed in a non-differential form as $E_L$ can be derived as a specific model. 

            $$f(\text{l}, \text{v}) = \frac{L_o(\textbf{v})}{E_L(\textbf{l})\overline{\text{cos}}\theta_i}$$

            So for evaluating radiance in a particular viewing direction, it's straightforward with n non-area light sources: 

            $$L_o(\text{v}) = \sum_{k = 1}^{n}f(\textbf{l}_k, \textbf{v}) \otimes E_L\overline{\text{cos}}\theta_{i_k}$$

            In order to describe an direction($\textbf{l}$ or $\textbf{v}$), the parameterization is to use two angles: elevation $\theta$ relative to the surface normal and rotation $\phi$ about the normal. This gives total four scalar variables in the general case of BRDF. \emph{Isotropic} BRDFs remain the same when the incoming and outgoing direction are rotated around the surface normal with same relative angles. They just have three scalar variables. 

            The BRDF is defined as radiance dived by irradiance, so its units are $\text{sr}^{-1}$. Intuitively,, the BRDF value is the relative amount of energy reflected in the outgoing direction, given the incoming direction. 

            Lights interaction with optical discontinuity will have various phenomena occur as discussed before. The phenomena called \emph{subsurface scattering} describe light that transmitted into the object may return and exit the surface with absorption and scattering. There are two circumstances when rendering: When the area covered by a pixel is relatively small compared to the distance between the entry and exit locations of subsurface scattering, BRDF can't be used. Instead \emph{bidirectional surface scattering reflectance distribution function} (BSSRDF)is used to exhibit large-scale subsurface scattering. 

            When a pixel covers a relatively large area, reflection, refraction and subsurface scattering can be approximated as happening at a single point, allowing for the usage of BRDF. This shows that whether a BRDF can be used depends both on the surface material and observation scale. 

            The derivation of BRDF assumes BRDF as a property of uniform surfaces, which is unreal. A function that captures BRDF variation based on spatial location is called a \emph{spatially varying BRDF}(SVBRDF) or \emph{SBRDF}. To handle transmission of light, two BRDFs and two BTDFs are defined for the surface, one for each side, and so make up the BSDF. In most situation, BRDF or SVBRDF is sufficient. \newline


            The laws of physics put two constraints on BRDF: the first one is \emph{Helmholtz reciprocity}
                $$f(\textbf{l}, \textbf{v)} = f(\textbf{v}, \textbf{l})$$

            In practice, BRDF will violate it without noticeable artifacts. 

            The second constraint is conversion of energy: the outgoing energy con not be greater that the income energy. In the real time rendering, strict energy conversion is not necessary, but approximate energy conversion is desired. 

            The \emph{directional-hemispherical reflectance} $R(\textbf{l})$ is a function related to the BRDF. It measures measures the amount of light coming from a given direction that is reflected at all, regardless of outgoing direction. Essentially, it measures the energy loss for a given incoming direction. 

            $$R(\textbf{l}) = \frac{dM}{dE(\textbf{l})}$$

            For non-area light, non-differential form can be used:

            $$R(\textbf{l}) = \frac{M}{E_L\overline{\text{cos}}\theta_i}$$

            Like BRDF, $R(\textbf{l})$ is expressed in RGB form. Besides, it is in range [0,1] so it can be considered as RGB color. Note that the restriction of energy conversion does not apply to BRDF with arbitrarily high values in certain directions. The relationship between directional-hemispherical reflectance and BRDF: 

            $$R(\textbf{l}) = \int_{\Omega}f(\textbf{l}, \textbf{v})\cos{\theta_o}d\omega_o$$

            where $\theta_o$ is the angle between \textbf{n} and \textbf{v}.

            The most simple BRDF model is Lambertian BRDF. It has a constant value and usually represents subsurface scattering. The directional-hemispherical is also a constant value:
            $$R(\textbf{l}) = \pi f(\textbf{l}, \textbf{v})$$
            
            The constant reflectance value of a Lambertian BRDF is commonly referred to as the \emph{diffuse color} $\textbf{c}_{\text{diff}}$: 
            $$f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi}$$
            
            So for lambertian shading equation: 

            $$L_o(\textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} \otimes \sum_{k = 1}^{n} E_{L_k}\overline{\text{cos}}\theta_{i_k}$$

            The $\frac{1}{\pi}$ is usually integrated into $E_{L_k}$.

            For visualized BRDFs with fixed incoming direction, the spherical terms terms reflect diffuse term as it goes to every direction. The \emph{reflectance lobe} represents the specular and the thickness of it represents the fussiness of the reflection. 

        \subsubsection{Surface and Body Reflectance}
            When using BRDF representations, surface phenomena are simulated as \emph{surface reflectance}, and the interior phenomena are models as \emph{body reflectance}(or \emph{volume reflectance}). The surface is a optical discontinuity and as such scatters light with reflections and transmissions but does not absorb any; The object's interior contains matter, which may absorb some of the light and this process is under the surface. It may also contain additional optical discontinuities that will further scatter the light. Surface reflectance is modeled by specular BRDF terms while body reflectance is modeled with diffuse term. 
        \subsubsection{Fresnel Reflectance}
            The interaction of light with a planar interface between two substances follows the \emph{fresnel reflectance}. Perfect flat surface is expected, while in practice a surface can be considered as perfectly flat with irregularities much smaller than the smallest light wavelength. 
            
            An optically planar interface between two substances will scatter light into ideal reflection direction and ideal refraction direction. 

            Note that the ideal reflection direction forms the same angle with the surface normal \textbf{n} as the incoming direction \textbf{l}. The amount of light reflected is described by the \emph{Fresnel reflectance} $R_F$. So the proportion of transmitted flux is $1 - R_F$ due to the energy conversion. However, radiance proportion of transmitted light is different due to the projected area and solid angle: 
            
            $$L_t = (1 - R_F(\theta_i))\frac{\sin^2{\theta_i}}{\sin^2{\theta_t}}L_i $$

            The refraction vector $r_i$ can be computed: 
            $$r_i = -\textbf{l} + 2(\textbf{n}\cdot\textbf{l})\textbf{n}$$

            The relationship between the $\theta_t$ and $\theta_i$ is described by Shell's Law with \emph{refraction index} or \emph{index of refraction}:
            $$n_i\sin{\theta_i} = n_t\sin{\theta_t}$$

            So for the transmitted radiance:

            $$L_t = (1 - R_F(\theta_i))\frac{n_t^2}{n_i^2}L_i $$

            The importance of Fresnel equation is description of the dependence of $R_F$, $\theta$, $n_1$ and $n_2$.\newline 

            \emph{External Reflection} is the case where light reflects from an object's external surface. In other word, the light is traveling from air to the object. 

            For rendering purpose, $R_F(\theta_i)$ is treated as RGB vector. $R_F(0^{\circ})$, sometimes called \emph{normal incidence}, can be thought of as the characteristic specular color of the substance. As $\theta_i$ increases, $R_F(\theta_i)$ intends to increase. When $\theta_i = 90^{\circ}$, the $R_F(\theta_i)$ reaches 1 for all frequencies. In rendering, the increase in reflectance at glancing angles is called \emph{Fresnel effect}.

            Besides the complexity of $R_F(\theta_i)$, they require refractive index values sampled over the visible spectrum, which make it difficultly to directly use in rendering. A simple approximation for most substances of Fresnel equation: 

            $$R_F(\theta_i) = R_F(0^{\circ}) + (1 - R_F(0^{\circ}))(1 - \overline{\text{cos}}\theta_i)^5$$

            The approximation performs not so well when $R_F(\theta_i)$ is not a monotonic. For precise value, it's better to pre-compute these value into a one-dimensional lookup texture. $R_F(0^{\circ})$ is the only control parameter here, which is convenient. It can be derived from object's refractive index: 
            $$R_F(0^{\circ}) = (\frac{n - 1}{n + 1})^2$$

            If refractive indices vary from frequencies, it's better to converted to RGB vector using methods described at 7.3\newline 

            For typical objects, according to their $R_F(0^{\circ})$, can be divided to insulators(or dielectrics), metals and semiconductors. Semiconductors too rare to put into discussion. 

            Most encountered materials are insulators with $R_F(0^{\circ})$ lower than 0.05. This makes fresnel effect more obvious. Besides, the optical properties of insulators rarely vary much over the visible spectrum, leading to colorless reflectance value. The light may go over further scattering and absorption; It may have internal reflection going through transparent objects. 

            Metals have high value of $R_F(0^{\circ})$, almost always 0.5 or above. The optical properties of insulators vary over the visible spectrum, leading to colorful reflectance value. The light will be immediately absorbed. 

            Internal reflection and External reflection have different refractive indices determine different Fresnel reflectance. \newline 

            \emph{Internal Reflection} occurs when light is travelling in the interior of a transparent object and encounters the object's surface. 

            The difference between external reflection and inter reflection is the \emph{critical angle} for \emph{total internal refraction}. The $R_F(\theta_i)$ curve of internal reflection is a compressed version of that of external reflection with same $R_F(0^{\circ})$. The compute to critical angle is straightforward: 
            $$\sin{\theta_c} = \frac{n_2}{n_1} = \frac{1 + \sqrt{R_F(0^{\circ})}}{1 - \sqrt{R_F(0^{\circ})}}$$

            The approximation for external reflectance can be used in internal reflectance when $\theta_t$ substitutes $\theta_i$.

        \subsubsection{Local Subsurface Scattering}
            For insulators, body reflectance needs to be taken into consideration. If the insulators are \emph{homogeneous}, with few internal discontinuities to scatter light, like glass; they partially absorb bur don't change their direction. 

            Most insulators are \emph{heterogeneous}, containing numerous discontinuities. These will cause light to scatter inside the substance. The light will be partially absorbed and those is not absorbed will re-emitted. We assume that the light is re-emitted from the same point at which it entered. This assumption is called \emph{local subsurface scattering}.
            
            The \emph{scattering albedo of} $\rho$ of a heterogeneous insulators is the ratio between the energy of the light that escapes a surface compared to the energy of the light entering into the interior of the material. $\rho$ is in range [0,1] and is modeled into RGB vectors. The bigger $\rho$ is, the more light scattered and the brighter the objects are visualized. 
            
            Since insulators transmit most incoming light rather than reflecting it at the surface, $\rho$ is more visually important than the $R_F(\theta_i)$. Since it results from a completely different physical process than the specular color, $\rho$ may have a completely different spectra distribution. 

            As introduced before, the diffuse term of BRDF in Lambertian is usually used to represent local subsurface scattering. Recall:  

            $$f_{\text{diff}}(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi}$$

            To set $\textbf{c}_{\text{diff}}$, we should first subtract specular term and get the diffuse term. If we have constant specular directional-hemispherical reflectance, it can be computed as :

            $$\textbf{c}_{\text{diff}} = (1 - \textbf{c}_{\text{spec}})\rho$$

            Only using $\rho$ to present $\textbf{c}_{\text{diff}}$, it just count specular term into subsurface scattering. 

            As fresnel reflectance represents the proportion of reflection, the diffuse term can be computed as:

            $$f_{\text{diff}}(\textbf{l}, \textbf{v}) = (1 - R_F(\theta_i))\frac{\rho}{\pi}$$

            It seems reasonable for the diffuse term is not affected by \textbf{v}. However, as it's implied by reciprocity and fresnel reflectance direction preference, the outgoing direction should be taken into consideration. 

            The assumption support both energy conservation and Helmholtz reciprocity: 
            $$f_{\text{diff}}(\textbf{l}, \textbf{v}) = k_{\text{norm}}(1 - R_{\text{spec}}(\textbf{l}))(1 - R_{\text{spec}}(\textbf{v}))\rho$$

            where $k_{\text{norm}}$ is a constant computed to ensure energy conversion. Given a specular BRDF term, using this equation to derive a matching diffuse term is not trivial since in general $R_{\text{spec}}$ does not have a closed form. 
        
        \subsubsection{Microgeometry}
            Surface detail modeled by a BRDF is \emph{microscale} -- smaller than the visible scale, or in other words, smaller than a single pixel. Since such \emph{microgeometry} is too small to be seen directly, its effect is expressed statistically in the way light scatters from the surface. 

            The most important visual effect of the microgeometry is due to the fact that many surface normals are present at each visible surface point. As normal can determine the outgoing direction, and the directions are somewhat random, it make sense to model them statistically as a distribution. For most surfaces, the distribution of microgeometry surface normals is a continuous distribution with a strong peak at the macroscopic surface normal. The tightness of this distribution determined by surface normal. 

            The visible effect of increasing microscale roughness is greater blurring of reflected environmental detail. It also can be seen that  statistical patterns in the many small highlights eventually become details in the shape of the resulting aggregate highlight.  

            For most surfaces, the distribution of the microscale surface normals is isotropic. While some surfaces have microscale structure that is \emph{anisotropic}, resulting in directional blurring of reflections and highlights. Some surfaces have highly structured microgeometry, resulting in interesting microscale normal distributions and surface appearance, e.g., fabric. 

            Other effects can also be important.\emph{Shadowing} refers to occlusion of the light source by microscale surface detail. \emph{Masking} refers to the visibility occlusion of microscale surface detail. If there is a correlation between this and surface normal, the distribution will change. As glancing angle $\theta_i$ increases, the irregularities will decrease. When $\theta_i \approx 90$, the surface will become optically smooth. This effect will be reinforced by Fresnel effect. 

            Light may experience inter reflections. Under the effect of Fresnel reflection, it tends to be attenuated and not noticeable in insulators. In metals, this is the source of diffuse reflection. Note that the reflection will make the light more colored. 

            In certain cases, microscale surface detail can affect body reflectance. If the scale of the microgeometry is large relative to the scale of subsurface scattering, then shadowing and masking can cause a \emph{retro-reflection} effect, where light is preferentially reflected back toward the incoming direction. This will result different brightness viewing from different directions. It will also tend to make the surface flatter. 

        \subsubsection{Microfacet Theory}
            BRDF models are based on a mathematical analysis of the effects of microgeometry on reflectance called \emph{microfacet theory}. The theory  is based on the modeling of microgeometry as a collection of \emph{microfacets}. Each microfacet is a tiny, flat Fresnel mirror. In microfacet theory, a surface is characterized by the distribution of the microfacet surface normal, which is defined by the surface's \emph{normal distribution function} (NDF) represented by $p()$. The NDF is defined as the probability distribution function of the microfacet surface normals. 
            
            The probability that the surface normal of a given microfacet lies within a infinitesimal solid angle $d\omega$ surrounding a candidate direction vector $n_{\mu}$ is equal to $p(n_{\mu})d\omega$. For a function to be a valid NDF, it needs to be \emph{normalized} so that it integrates to 1 over the sphere. As mentioned, most surfaces have NDFs that show a strong peak at the macroscopic surface normal $n$.

            Microfacet theory focuses on modeling the first-bounce specular reflection so microfacet-derived BRDF terms are paired with a diffuse term. Shadowing and masking are done in specific way. 

            As microfacets are assumed to be ideal mirrors, only those microfacets that reflect \textbf{l} tp \textbf{v} will participate in the reflection. It means the normal vector is aligned with \emph{half vector} \textbf{h}.

            The fraction of active microfacets is equal to $p(\textbf{h}$. The reflectance will also depend on Fresnel reflectance $R_F(\alpha_h)$, where $\alpha_h$ is the angle between \textbf{l} and \textbf{h}. 

            shadowing and masking are accounted for by introducing a \emph{geometry factor} or \emph{geometrical attenuation factor} G(\textbf{l}, \textbf{v}). It's in range [0,1], representing how much light remains after shadowing and masking are applied. The shadowing and masking are not correlated with microfacet orientation. A example of derivation:

            $$f(\textbf{l}, \textbf{v}) = \frac{p(\textbf{h})G(\textbf{l},\textbf{v})R_F(\alpha_h)}{4k_p\overline{\text{cos}}\theta_i\overline{\text{cos}}\theta_o}$$
            
            $$k_p = \int_{\Omega}p(\textbf{h})\cos{\theta_h}d\omega_h$$

            The NDF $p()$ is usually \emph{isotropic} -- rotationally symmetrical about the macroscopic surface normal \textbf{n}. In this case, it's a function of just one variable: $\theta_h$.

            Another, more complete surface representation is called \emph{height correlation}. It can correlate the normal with shadowing and masking while is too costly. 

        \subsubsection{Half Vector VS. Reflection Vector}
            Instead of $\theta_h$, some BRDFs use the angle $\alpha_r$ between the reflection vector $\textbf{r}_i$ and the view vector \textbf{v}. $\theta_h$ has physical interpretation -- it shows to what extent the active microfacet normals diverge from the overall macroscopic surface normal -- while $\alpha_r$ hasn't. 

            While the highlight position will be the same, the shape of the highlight may differ. In reality, the half vector based BRDFs is closer to reality. 
    
    \subsection{BRDF models}

        \subsubsection{Special Notes}
            Last subsection it introduces physical based BRDF: microfacet BRDF. It's derived from the actual physics. When modeling the empirical model, it's better to make it more corresponding to the microfacet BRDF to form the physical based one. 
            
            Another perspective for Blinn-Phong Shading evolution: 

            The original form: 
            $$I = k_aI_a + k_d(\textbf{n}\cdot \textbf{l})I_d + k_s(\textbf{r} \cdot \textbf{v})^{\alpha})I_s$$
            
            Introduced in BRDF: 

            $$L_o(\textbf{v}) = f(\textbf{l}, \textbf{v}) \otimes E_L\overline{\text{cos}}\theta_{i}$$

            The $E_L\overline{\text{cos}}\theta_i$ is the irradiance, which symbolizes $I$. The environmental part is integrated into the diffuse term to match physical rules. Note that diffuse color and specular color describes the ratio of light energy in a spectrum form. The difference between them is that specular term is derived from diffuse term with the influence of viewing angle. These two term is different in the viewing perspective; Equation for this:

            $$f(\textbf{l}, \textbf{v}) = \textbf{c}_{\text{diff}} + \textbf{c}_{\text{spec}}\overline{\text{cos}}^m\alpha_r$$

            For the half-vector version: 

            $$f(\textbf{l}, \textbf{v}) = \textbf{c}_{\text{diff}} + \textbf{c}_{\text{spec}}\overline{\text{cos}}^m\theta_h$$            

            The diffuse term in Lambertian is a constant. Considering from hemispherical-directional reflectance, to keep $\textbf{c}_{\text{diff}}$ as a color and energy conversation(This is a easy way to normalize)

            $$R(\textbf{l}) =  \pi\textbf{c}_{\text{diff}} \quad\Rightarrow\quad \textbf{c}_{\text{diff}} = \frac{R(\textbf{l})}{\pi}$$
            
            For specular term, comparing with microfacet model in most use: 
            $$f(\textbf{l}, \textbf{v}) = \frac{p(\textbf{h})G(\textbf{l}, \textbf{v})R_F(\theta_i)}{4\overline{\text{cos}}\theta_i\overline{\text{cos}}\theta_o}$$

            The $\textbf{c}_{\text{spec}}$ matches $R_F(\theta_i)$. As NDFs can present roughness, it requires normalization of the NDF, which means the sum of projected microfacet are to be equal to the macro projected area along \textbf{v}(This is ensured the integral is equal to 1)

            $$1 = \int_{\Omega}K\overline{\text{cos}}^m\theta_hd\omega_i$$
            $$K = \frac{m + 2}{2\pi}$$

            We set the visibility term to $\frac{1}{4}$. So the outcome is
            
            $$f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} + \frac{m + 2}{8\pi}R_F(\alpha_h)\overline{\text{cos}}^m\theta_h $$

        Symbols in use: 

        $\theta_i$: angle between \textbf{l} and \textbf{n}

        $\theta_o$: angle between \textbf{v} and \textbf{n}

        $\theta_h$: angle between \textbf{h} and \textbf{n}
        
        $\alpha_r$: angle between \textbf{v} and $\textbf{r}_i$

        $\alpha_h$: angle between \textbf{l} and \textbf{h}


        \subsubsection{Main Text}

        BRDF models fall into two broad groups: those based on physical theory and those that are designed to empirically fit a class of surface types. 

        Since empirical BRDF models tend to be simpler, they are commonly used in real-time rendering. The first such model used was Lambertian or constant BRDF. The BRDF form: 
        $$
            f(\textbf{l}, \textbf{v}) =
            \begin{cases}
                \frac{\textbf{c}_{\text{diff}}}{\pi} + \frac{\textbf{c}_{\text{spec}}\overline{\text{cos}}^m\alpha_r}{\pi\overline{\text{cos}}\theta_i} & \text{where } \theta_i > 0 \\
                
                0 & \text{where } \theta_i \leqslant 0 
            \end{cases}
        $$
            
        Fortunately, $\textbf{c}_{\text{diff}} = R_{\text{diff}}(\textbf{l})$. As$\textbf{c}_{\text{diff}}$ is restricted to values between 0 and 1, so it can be treated as color. For specular term, as $R_{\text{spec}}$ is unbounded. This is caused by the division by $\overline{\text{cos}}\theta_i$. When removing from it from the BRDF, it's simpler and more physically plausible.
        $$f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} + \frac{\textbf{c}_{\text{spec}}\overline{\text{cos}}^m\alpha_r}{\pi}$$
        
        In this term, when $\theta_i = 0$, $R_{\text{spec}_{\text{max}}} = \frac{2\textbf{c}_{\text{spec}}}{m + 2}$. In order to make $\textbf{c}_{\text{spec}} = R_{\text{spec}}$: 

        $$f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} + \frac{m + 2}{2\pi}\textbf{c}_{\text{spec}}\overline{\text{cos}}^m\alpha_r$$

        This process of multiplying a BRDF term by a value to ensure that parameters are  equivalent to reflectance values is called \emph{BRDF normalization}. The value by which the term is divided is called a \emph{normalization factor}, and the resulting BRDF is referred to as a \emph{normalized BRDF}. The parameter being normalized can be treated as color. In this case, the maximum value of the $R_{\text{spec}}$ is closely related to the Fresnel reflectance at normal incidence. As the equation don't account for the fresnel effect, it's wise to use a value between white and $R_F(0^{\circ})$. For energy conservation, it's guaranteed by ensuring $\textbf{c}_{\text{diff}} + \textbf{c}_{\text{spec}} \leqslant 1$

        Another important effect of normalization is that now the $m$ parameter is independent of reflectance and only controls surface roughness. For the original one, when $m$ changed, the integral of all view direction's reflected energy will be decreased as $m$ increases. This is bad because the reflectance defines the \emph{perceived} specular color. So for texture editing with both smoothness and color, it's hard to choose a proper $m$. 

        When normalized, the reflectance is constant. Recall that although reflectance values are in [0,1], but BRDF values can exceed 1. In particular scene, the highlight follows the property that it becomes brighter as it narrows using normalized BRDF. The original BRDF doesn't have the property. It's hard and unintuitive to adjust to the same effect with original one. 
        
        Normalized BRDFs yield the ability to intuitively control physically meaningful parameters, enabling easy authoring of realistic materials. Note that the physically derived BRDFs do not require normalization, since they have physical parameters by construction. 

        The purpose of the normalization is to guarantee energy conservation. So in Lambertian BRDF, we will modify $\textbf{c}_{\text{diff}}$ to the maximum of $R_{\text{spec}}$
        
        The presentation can be converted to half-vector-based variant with normalization (\emph{Blinn-Phong BRDF}): 

        $$f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} + \frac{m + 8}{8\pi}\textbf{c}_{\text{spec}}\overline{\text{cos}}^m\theta_h$$ 

        Note that to get a similar highlight, the value of $m$ used in the Blinn-Phong BRDF should be about four times that used for the Phong BRDF. The Blinn-Phong BRDF can also be cheaper to evaluate than the Phong BRDF in some cases. 

        Anther advantage is that the form is similar to a microfacet BRDF. This similarity means that the various expressions can be interpreted using microfacet theory. For example, the cosine power term can be interpolated as a NDF. It's also clear that the BRDF can be extended to include Fresnel effect by simply replacing $\textbf{c}_{\text{spec}}$ with $R_F{\alpha_h}$. It's also acceptable to replace the diffuse term with other diffuse terms. 

        $$f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} + \frac{m + 8}{8\pi}R_F(\alpha_h)\overline{\text{cos}}^m\theta_h$$ 
        
        This equation is enough for most real-time rendering needs. However, there are some phenomena it does not model and the need or a different BRDF will arise. For example, the cosine term NDF used (sometimes called a \emph{Phong lobe}) can be replaced. While the hardest part is computing a new normalization factor for the specular term. 
        
        Various other isotropic NDFs have been proposed. \emph{multi-scale roughness} results in NDF curves that appear to be combinations of simpler curves. 

        Anisotropic NDFs can be used as well. Anisotropic NDFs can't be evaluated just with the angle $\theta_h$; additional orientation information is needed. In general case, the half vector \textbf{h[} needs to be transformed into the tangent space(\emph{local frame}).

        For surfaces that are extremely anisotropic surfaces and best modeled as tightly packed one-dimensional lines, the Kajiya-Kay BRDF is employed. The basic concept is based on the observation that a surface composed of one-dimensional lines has an infinite number of normals at any given location, defined by the \emph{normal plane} perpendicular to the tangent vector \textbf{t}. When normal is selected out, it's identical to the original Phong BRDF. For diffuse and specular term, different normals are selected. The diffuse term uses the projection of light vector \textbf{l} onto the normal plane, and the specular term uses the view vector \textbf{v} projected onto the normal plane. 

        $$\overline{\text{cos}}\theta_i^{'} = \sqrt{1 - (\textbf{l}\cdot\textbf{n})^2}$$

        $$\overline{\text{cos}}\alpha_r^{'} = \text{max}(\sqrt{1 - (\textbf{l}\cdot\textbf{t})^2}\sqrt{1 - (\textbf{v}\cdot\textbf{t})^2} - (\textbf{l}\cdot\textbf{t})(\textbf{v}\cdot\textbf{t}), 0)$$

        The substitution of angle can be used in normalized version. It's also better to use half-vector \textbf{h} to find the specular shading normal. 

        
        \bigskip 
        \bigskip


        Comparing the modified Blinn-Phong specular term with the full microfacet BRDF. We first use the cosine term as NDF:

        $$f(\textbf{l}, \textbf{v}) = (\frac{G(\textbf{l}, \textbf{v})}{\overline{\text{cos}}\theta_i\overline{\text{cos}}\theta_o})(\frac{m + 2}{8\pi}\overline{\text{cos}}^m\theta_h)R_F(\alpha_h)$$

        Aside from the difference in normalization, the most significant difference between the two BRDFs is the leftmost term. The geometry factor models shadowing and masking, and the cosine factors model the foreshortening of the surface with respect the the light or camera based on surface area. This combined term is the \emph{visibility term}.

        The modified Blinn-Phong BRDF has a visibility term equals to 1, which will bring: 
        $$G_{BF}(\textbf{l}, \textbf{v}) = \overline{\text{cos}}\theta_i\overline{\text{cos}}\theta_o$$
            
        The visibility term tend to decrease at glancing angles. However, the decrease happens rapidly as the light or view vector diverges from the surface normal according to the implied relationship, causing Blinn-Phong BRDF to appear dark at glancing angles compared to real surfaces. 

        The visibility term of the Ward BRDF is equal to $\frac{1}{\overline{\text{cos}}\theta_i\overline{\text{cos}}\theta_o}$. The Ward BRDF also does not include a Fresnel term. This visibility term is not recommended as it is unbounded at a glancing angle.   

        A visibility term between no visibility term and unbounded visibility term is preferred. They are designed primarily to ensure reasonable behavior at glancing angles. Those included in physically based microfacet BRDF models have explicit geometry factors designed to model shadowing and masking, divided by the cosine terms to model foreshortening. The most well-known one is: 

        $$G_{TS}(\textbf{l}, \textbf{v}) = \text{min}(1, \frac{2\overline{\text{cos}}\theta_h\overline{\text{cos}}\theta_o}{\overline{\text{cos}}\alpha_h}, \frac{2\overline{\text{cos}}\theta_h\overline{\text{cos}}\theta_i}{\overline{\text{cos}}\alpha_h})$$

        There is a simple approximation the entire visibility using this geometry factor: 

        $$\frac{G_{TS}(\textbf{l}, \textbf{v})}{\overline{\text{cos}}\theta_i\overline{\text{cos}}\theta_o} \approx \frac{2}{1 + \textbf{l}\cdot\textbf{v}} = \frac{4}{\textbf{h}^{'}\cdot\textbf{h}^{'}} = \frac{1}{\cos^2{\alpha_h}}$$

        $$\textbf{h}^{'} = \textbf{l} + \textbf{v}$$

    \subsection{BRDF Acquisition and Representation}
        In most real-time applications, BRDFs are manually selected and their parameters set to achieve a desired look. However, they are measured from actual surfaces. 
        \subsubsection{Acquiring BRDFs}
            goniometer, imaging bidirectional reflectometer, and image-based methods have being used.
        \subsubsection{Representations for Measured BRDFs}
            The result of BRDF capture is a large, densely sampled four-dimensional table. The acquisition process also introduces noise into the data. So the raw data must be converted into some other representation. 

            One common method is to select an analytic BRDF and fit its parameters to the measured data. It's not so simple to choose a suitable BRDF, a weighted sum of multiple BRDF terms is optional with hight cost. 

            Another way to represent BRDFs is to project them onto \emph{orthonormal bases}, such as spherical harmonics, spherical wavelets, etc. These techniques are focused on representing the shape of the BRDF as a generic function. 

            As four-dimensional functions, BRDFs can also be approximated, or \emph{factored}, into a sum of products of lower-dimensional functions. It these function are one or two dimensional, they can be stored in textures. The parameterization of the lower-dimensional functions needs to be chosen with care so as to minimize the number of functions needed. 

            These factorization techniques have two limitations: They are not suitable for measured SVBRDF, since a separate factorization process needs to be performed for each location, and the results can not easily be edited. a \emph{shade tree} is created to combine the textures and produce the final shaded pixel color. 
        
        \subsection{Implementing BRDFs}
            In many rendering systems, the $\frac{1}{\pi}$ factor is folded into $E_L$ for convenience. 
            
            The most straightforward way to evaluate any BRDF model that has an analytic expression, as opposed to being represented by a factorization or a sum of basis functions, is to simply evaluate the expression in a shader for each light source. It has best visual effect to do the whole calculation in pixel shader while expensive. Spatially varying BRDFs can be encoded in textures. The cosine value can be derived by the dot product of normalized vector. 

            In principle, BRDF equations apply in the \emph{local frame} of the surface in the \emph{local frame} of the surface defined by the vectors \textbf{n}, \textbf{t}, \textbf{b}. Normal mapping actually perturbs the entire local frame, while isotropic BRDF rarely require other than normals. Anisotropic BRDFs introduce the possibility of the per-pixel modification of the tangent direction -- use of textures to perturb both the normal and tangent vectors is called \emph{frame mapping}.

            BRDFs using non-parametric representations are more difficult to render efficiently. 

        \subsubsection{Mipmapping BRDF and Normal Maps}
            normal maps can't adopt common linear interpolation like bilinear filtering and mipmapping. Remember the BRDF is a statistical description of the effects of subpixel surface structure. As the distance between surface and camera increases, the surface is handled by BRDF rather than bump mapping. This transition is tied to the mipmap chain. Macro--geometry; Meso--bump mapping; Micro--BRDFs. 

            The normal maps and surrounding NDF represents the underlying structure. Each texel in the normal map, combined with the cosine power, can be seen as collecting the NDF across the surface area covered by the texel. 

            The ideal representation of this surface at a lower resolution would be exactly represent the NDFs collected across largest surface areas.
            
            However, as presentation at low resolutions is the responsibility of higher level mipmap, one result is averaging and re-normalizing the normals in the normal map, and NDF experience same integration.This will lead to incorrect appearance.
            
            We can't represent the ideal NDFs directly with the Blinn-Phong BRDF. However, with a gloss map, the cosine power can be varied from texel to texel. For each ideal NDF, we find the rotated cosine lobe that matches it most closely. We store the center direction of this cosine lobe in the normal map, and its cosine power in the gloss map.  

            (For cosine lobe method, it isolates the effect of controlling parameter $m$[NDF] and integrate normals in the mipmap hierarchy)

            In general, the ideal is to filter the entire surface appearance, represented by the BRDF. 

            A clever observation is that  if normals are averaged and not re-normalized, the length of the averaged normal correlates inversely with the width of the normal distribution. So for aggregate $m$ value in the gloss map for mipmap: 

            $$m^{'} = \frac{\|\textbf{n}_a\|m}{\|\textbf{n}_a\| + m(1 - \|\textbf{n}_a\|)}$$

            Where $\|\textbf{n}_a\|$ is the length of the averaged normal. This feature is particularly useful for dynamically generated normal maps on the fly. This pattern doesn't allow for compression but gloss map can be encoded. 
        
    \subsection{Combining Lights and Materials}
        Lighting computations occur in two phases. In the \emph{light phases}, it is relevant to the \textbf{l} and $E_L$. In the \emph{material phase}, the BRDF parameters and surface normal are found via interpolation or texturing to evaluate  the BRDF value.

        The combinations with different lights and materials create numerous pixel shader combinations. In addition, the fact that both the light and material phase need to be repeated for each light source  can lead to great inefficiencies. As the limitation of dynamic branches, a common solution to this problem is to write a single large shader that is compiled multiple times, using language features or code pre-processors to create different shader combinations. However, it's impossible when combination is too much. 

        One tactic commonly used by game developers is to define a fixed, cheap lighting model and approximate the actual lighting environment of each object to fit the model. Other kind of light source will be converted into defined light sources to cut the number of combination. However, it introduces visual artifacts. 

        \subsubsection{Multipass Lighting}
            Multipass lighting is another solution to the problem of combining lights and materials. Here the idea is to process every light in a separate rendering pass, using the hardware's additive blending capabilities to sum the results in the frame buffer. The list of affecting each object can be narrowed by bounding volume intersection. After the set of lights is determined, each object is rendered once for each light. This control the amount of shader. 

            There are some practical issue: The use of a lot of1 memory bandwidth; The mesh needs to be processed multiple times by the vertex shader, and any pixel shader computations that produce results used by the lighting process need to be repeated for each light source. The summing of light may cause artifacts as well. 
        
        \subsubsection{Deferred Shading}
            The basic idea behind deferred shading is to perform all visibility testing before performing any lighting computations. For the first pass, values included the $z$-depth, normal, texture coordinates, and material parameters are saved to multiple render targets accessed by the pixel shader program with the sorting of $Z$-buffer. The stored buffer are commonly called \emph{G-buffers}. 

            After this, separate pixel shader programs are used to apply lighting algorithms or visual effects. A pixel shader program is applied with a viewport-filling quadrilateral to drive each computation.  One advantage is fragment shading is done only once per pixel and so has a predictable upper bound on rendering cost. Geometric transformation is done only once, in the initial pass, and any computations needed for lighting are done only once. 

            Another important advantages has to do with programming and assets management costs. It allows a strong separation between lighting and material definition as they are abstracted. And the time-consuming process of determining exactly which lights affect which objects is avoided. 

            The drawbacks: The video memory requirements and fill rate costs for the G-buffer are significant;  Antialiasing is limited; Transparent objects must be drawn using screen-door transparency or rendered into scene after all deferred shading passes are done. 
    \newpage

\section{Area and Environmental Lighting}
    This section focuses on indirect lighting. Other from the focus on viewing in the last section(focus on BRDF $f(\textbf{l}, \textbf{v})$), this section is considered about the lighting perspective ($E_L\cos{\theta_i}$). 
    \subsection{Radiometry for Arbitrary}

        We assume that light can be in any direction; Recall the definition of radiance, as irradiance adds up all incoming light:

        
        $$E = \int_{\Omega}L_i(\textbf{l})\cos{\theta_i}d\omega_i$$

        $\Omega$ represents a hemisphere direction of light. Combining generalized BRDF :
        
        $$L_o(\textbf{v}) = \int_{\Omega}f(\textbf{l}, \textbf{v}) \otimes L_i(\textbf{l})\cos{\theta_i}d\omega_i$$

        For parameterization version:
        $$L_o(\theta_o, \phi_o) = \int_{\phi_i = 0}^{2\pi}\int_{\theta_i = 0}^{\frac{\pi}{2}}f(\theta_i, \phi_i, \theta_o, \phi_o) L(\theta_i, \phi_i)\cos{\theta_i}\sin{theta_i}d\theta_i d\phi_i$$

    \subsection{Area light}
        Other than direction light, which the surface is illuminated from one direction only for every point, the area light \emph{subtend} a solid  angel of $\omega_L$ as the light source is considered to have non-zero area. For smooth surface with small solid angle, the BRDF used for directional can be good approximation. The equation for the approximation is:

        $$L_o(\textbf{v}) = \int_{\omega_L}f(\textbf{l}, \textbf{v}) \otimes L_L\overline{\text{cos}}\theta_id_{\omega_i} \approx f(\textbf{l}_L, \textbf{v}) E_L\overline{\text{cos}}\theta_{i_L}$$ 

        For the special case of Lambertian surface to a single wavelength, as 
        
        $$L_o(\textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi}E$$

        Under this circumstances we focus on the irradiance:

        $$E = \int_{\omega_L}L_L\overline{\text{cos}}\theta_id\omega_i \approx E_L\overline{\text{cos}}\theta_{i_L}$$

        \emph{Net irradiance}, compared to the normal irradiance, particles are counted as positive or negative depending on which way it pass through. The equation: 
            $$\overline{E}(\textbf{p}, \textbf{n}) = E(\textbf{p}, \textbf{n}) - E(\textbf{p}, -\textbf{n})$$
        
        \emph{Vector irradiance} can convert the total irradiance of area lighting to a irradiance of a point or directional light accurately, as presented by three basis:

        $$\textbf{e}(\textbf{p}) = \int_{\Theta}L_i(\textbf{p}, \textbf{l})\textbf{l}d\omega_i$$

        So the relationship between vector irradiance and net irradiance: 

        $$\overline{E}(\textbf{p}, \textbf{n}) = \textbf{n} \cdot \textbf{e}(\textbf{p})$$

        When the negative part of net irradiance is 0:
        $$E(\textbf{p}, \textbf{n}) = \textbf{n} \cdot \textbf{e}(\textbf{p})$$
        
        This can be a solution to area light with no negative component of net irradiance. For mixed light, if all light sources have the same spectrum distribution, they will have same light distribution. A light color $\textbf{c}_L$ is introduced to the original equation with $\textbf{e}(\textbf{p})$ is a matrix; 

        This forms just apply for the Lambertian surface with no light source under the horizon. 

        The smoothness of surface matters for the different occlusion effects for sole-direction light or a light cone, as the latter will have a continuous highlight edge. Area light will have an obvious and sharp highlight on the surface compared to a tiny spot of the directional light.
    
    \subsubsection{Ambient Light}
        The current equation doesn't count the difference in the direct light and indirect light. Ambient light is the simplest way to define indirect light as it shares the same constant value $L_A$.

        For lambertian surface as $f(\textbf{l}, \textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi}$: 

        $$L_o(\textbf{v}) = \frac{\textbf{c}_{\text{diff}}}{\pi} \otimes L_A\int_{\Omega}\cos{\theta_i}d\omega_i  = \textbf{c}_{\text{diff}} \otimes L_A$$
           
        For arbitrary BRDF: 
        $$
            L_o(\textbf{v}) = \int_{\Omega}f(\textbf{l}, \textbf{v})(L_A + L_i(\textbf{l}))\cos{\theta_i}d\omega_i
        $$

        We define the \emph{ambient reflectance $R_A$(\textbf{v})}

        $$R_A(\textbf{v}) = \int_{\Omega}f(\textbf{l}, \textbf{v})\cos{\theta_i}d\omega_i$$

        In rendering, \emph{ambient color} represents ambient reflectance in RGB form: 
        $$
        L_o(\textbf{v}) = \textbf{c}_{\text{amb}} \otimes L_A  + \int_{\Omega}f(\textbf{l}, \textbf{v}) \otimes L_i(\textbf{l})\cos{\theta_i}d\omega_i
    $$

        Current discussion of ambient light ignores occlusion. 
    
    \subsection{Environment Mapping}
        From the viewing angle perspective, the reflected view vector $\textbf{r}$ is :

        $$\textbf{r} = 2(\textbf{n} \cdot \textbf{v})\textbf{n} - \textbf{v}$$

        If the incoming radiance $L_i$ is only dependent on direction, it can be stored in an \emph{environment map}. The texel coordinates are derived from the reflected view vector. Environment mapping is worse for flat surface as reflected view vector changes just a little bit and a small area of textures will be magnified to the surface. It's straightforward to use normal map and environment map together. 

        \subsubsection{Blinn and Newell's Method}
            This uses a simple spherical projector functions on the reflected view vector, and this brings uniform are distribution and pole artifacts. 
        \subsubsection{Sphere Mapping}
            This is different from spherical projector. Rather than use spherical coordinates, it presents a realistic-appearance sphere derived from sometimes ray-tracing and find one point on the sphere with the normal. This sphere is represented in a view base as view vector is along one of the axis. 

            In the view base, with reflected view angle $(r_x, r_y, r_z)$ on a unit sphere:

            $$m = \sqrt{r_x^2 + r_y^2 + (r_z + 1)^2}$$
            $$n = (\frac{r_x}{m}, \frac{r_y}{m}, \frac{r_z + 1}{m})$$
            $$u = \frac{n_x}{2} + 0.5, \quad v = \frac{n_y}{2} + 0.5$$

            The sphere mapping is valid for one direction. Changing sphere mapping will bring more artifacts as some area will be changed. 
        
        \subsubsection{Cubic Environment Mapping}
            The \emph{cubic environment map} is obtained by placing the camera in the center of the environment and then projecting the environment onto the sides of a cube positioned with its center at the camera's location along with the viewing direction.
            

        
\end{document}

