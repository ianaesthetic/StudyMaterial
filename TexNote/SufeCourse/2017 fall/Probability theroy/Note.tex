\documentclass[10pt, a4paper]{article}
    \title{Note}
    \author{ianaesthetic}
\usepackage{indentfirst, amsmath, fontspec, listings, xcolor, amssymb}
\usepackage{xeCJK}
\setCJKmainfont{STFangsong}

\usepackage{titlesec}
\newCJKfontfamily\sectionFont{Microsoft YaHei} 
\titleformat*{\section}{\Large\sectionFont}
\titleformat*{\subsection}{\large\sectionFont}
\titleformat*{\subsubsection}{\sectionFont}
\newfontfamily\consolas{Consolas}
\lstset{numberstyle = \small\consolas, basicstyle=\small\consolas}

\newfontfamily\consolas{Consolas}
\lstset{numbers = left, numberstyle = \small\consolas, basicstyle = \small\consolas}

\XeTeXlinebreaklocale "zh"
\XeTeXlinebreakskip = 0pt plus 1pt

%Cspell: ignore notin

\begin{document}
    \section{}
        可能性为0的时间不一定时不可能事件，设想：在圆内找随机找到任意一个点的概率都为0，但是不是不可能事件，

        给定一个随机试验，对于这个试验的样本空间的描述不止一种。    

    \section{条件概率与事件的独立性}
        定义：设A，B为两个事件，且 $P(A) > )$ 则称

        $$P(B|A) = \frac{P(AB)}{P(A)}$$
        
        为事件A发生条件下B发生的条件概率

        注意$P(AB)$和$P(B|A)$的差别。并且注意这两种情况的不同措辞 。 

        计算方法：
            如果A发生之后，样本空间能够重新很清楚的得出，那么就用古典概型来计算。否则使用公式来进行计算（公式中的所有值都是依照于原来的样本空间来计算的）。条件概率同样适用于概率的性质, 是已知原因寻求结果。

            $$P(AB) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$$ 

            上面的式子就是概率的乘法定理，用来计算复杂事件的概率,  下面时推广。   
            $$P(A_1A_2\cdots A_n) = P(A_1)P(A_2|A_1)P(_3|A_1A_2)\cdots P(A_n|A_1\cdots A_{n-1})$$

            波利亚罐子模型：两种球，取球时会加入某数量个同色球。这是一个简化的传染病模型。注意在这个模型中复杂事件中使用惩罚概率。 

            在描述事件的时候要尽量做到周全完善，比如「所取两件中至少有一件是不合格品」而不是「所取的两件中有一件是不合格品」。
            \subsection{事件的独立性}
                当A，B是随机事件的时候，$P(A) > 0$, 
                $$P(B) = P(B|A)$$
                
                则称事件B独立于事件A。

                独立性是相互的，及A独立于B则B也独立与A， 以此为基础定义A和B相互独立，当且仅当以下条件满足：
                    $$P(AB) = P(A) \cdot P(B)$$
                注意当P(A) = 0 的时候，则一定A和B相互独立。判断A B事件是否相互独立，一定要按照定义进行计算。相互独立的本质是一个事件的发生对于另外一个事件发生的概率没有任何影响。

                如果A，B相互独立,则$\bar{A}$与B，A与$\bar{B}$,$\bar{A}$和$\bar{B}$相互独立, 可以通过这个互相判断是否事件是否相互独立。

                注意「A B相互独立」和「A B互不相容」的区别：互不相容是集合关系，表示交集为空。相互独立表示的是一个数值相等的概率关系。
                    $$if\ P(A) > 0, P(B) > 0, AB = \phi$$
                    $$P(AB) = 0, P(A) \cdot P(B) != 0$$ 
                \subsubsection{多个事件独立性}
                    n个事件要相互独立，则要求所有的任意两个事件之间都相互独立。
    \subsection{全概率公式和Baayes 公式 }
        \subsubsection{分割}
            定义：设$A_1, A_2, \cdots, A_n$满足
            $$A_iA_j = \phi, P(A_i) = 0, i !=j $$
            $$$$

        \subsubsection{全概率公式}
            公式：设$A_1, A_2, \cdots, A_n$是一个分割，$B$为任意一个事件，则
                    $$B = \bigcup_{i = 1}^{n}A_iB$$
            显然 $A_iB$互不相容，于是
            $$P(B) = P(\bigcup_{i = 1}^{n}A_iB) = \sum_{i = 1}^{n}P(A_iB) = \sum_{i = 1}^{n}P(A_i)\cdot P(B|A_i)$$
            
            全概率公式是一个概率加法定理和乘法定理的结合，可以将事件进行分解成简单概率的总和。全概率事件的实质是不同事件$A_1, A_2, \cdots A_n$导致了B事件发生的概率的综合。
            
            在知道概率递推公式的情况下，可以使用数学归纳法取证明。 如使用数学归纳法计算抽签模型的概率的时候: 

            设定事件$A_i$为「在第i次抽取到白球」，而数学归纳法假定$P(A_i) = \frac{a}{a + b}$
            \begin{equation*}
                P(A_{i + 1}) = P(A_{i + 1}|A_i) + P(A_{i + 1}| \bar{A}_1)
            \end{equation*}
            $$P(A_{i + 1} | A_1) = P(A_2 | A_1),\quad P(A_{i + 1}|\bar{A}_1) = P(A_2|\bar{A}_1)$$
        \subsection{Bayes 公式}
            设$A_1, A_2, \cdots, A_n$是一个分割，$B$为任意一个事件，$P(B) > 0$, 则
            $$P(A_i|B) = \frac{P(A_i)P(B|A_i)}{\sum_{i = 1}^{n}P(A_i)\cdot P(B|A_i)}$$

            内涵：全概率公式的逆用，相当于分割其中的某一个事件相比于所有事件对于事件$B$的贡献的占比。表示结果一定的情况下，求导致原因的概率。不用执着于直接看出来是Bayes公式，在计算条件概率的时候会自然想到全概率公式。在公式之中，我们认为$P(A_i)$是验前概率，$P(A_i|B)$是验后概率，是对于新信息出现时对于概率的修正(例子：用Bayes公式解释狼来了）。

            重要的例子：习题集 P29, 13; P27, 7;
        \subsubsection{贝努利模型}
            n次相互独立的试验：对于随机试验，$E_1, E_2, \cdots, E_n$，如哦任何一次试验结果的发生都不依赖于其他试验的结果。注意试验独立性和事件独立性的区别。

            n次重复独立试验：将随机试验E重复进行n次，如哦每次试验样本空间相同，有关事件的概率保持不变，且各次试验相互独立。

            贝努利模型： 设随机试验满足

                \quad 1. 在相同条件下进行n次重复试验

                \quad 2. 每次试验两种可能结果，$A$和$\bar{A}$

                \quad 3. 在每次实验中A发生的概率不变 $P(A) = p$

                \quad 4. 各次试验相互独立
            
                则成为n重贝努利试验。

            定理：对贝努利模型，每次试验A发生的概率是$p$，则n次试验中A恰好发生K次的概率为
            $$B(k, n, p) = C_n^kp^k(1 - p)^{n - k}, k = 0, 1, 2 \cdots n$$
            $$\sum_{k = 0}^{n}C_n^kp^k(1 - p)^{n - k} = 1$$
            
            例题： 书p29, 例19, 例20;

            小概率事件： 在一次实验中发生的概率很小的事件，称为小概率事件。一般取在0.01或者0.0以下的值。 

            小概率原理：小概率事件在一次实验中几乎不可能发生，而某一概率很小的事件，居然在某一次实验发生了了，那么要怀疑原来概率的正确性。
        \subsubsection{章末习题}
            P31, 9;11 

            数学归纳法的一般形式和使用数学归纳法证明抽签模型类似，在证明$n+1$次的概率时，可以化为在第一次实验已经展开的情况下的$n+1$的概率，这时候相对于第一次已经发生时$n+1$时就被视为第n次，可以套入假设。 

            对于n次试验后的概率讨论，可以先从1开是找找规律。

            习题集 P29 6, 13, 24

    \section{(╯‵□′)╯︵┻━┻}
        \subsection{随机变量及其分布}
            在实际问题中，随机的试验的结果可以用数量来表示，由此产生了随机变量的概率。有些试验，试验结果或直接为数值，或者直接数值化。这里引入实值函数的概念：从样本空间映射到实轴上。

            定义：设随机试验的样本空间为$\Omega = \{\omega\}$，如哦对于每一个$\omega$
            
            对于随机变量的研究时包含了所有相对应的随机试验的研究。

            随机变量的分布函数：设X是一个随机变量，x是一个任意实数，概率$P(X \leqslant x)$是x的函数，则称此函数为随机变量X的随机函数，记为$F(x)$,即
            $$F(x) = P(X \leqslant x)$$

            使用$X \leqslant x$是他能够比较好的表述各种其他的区间，而且可以只拥有一个自变量。对于其他区间：
            $$P(X > x) = 1 - F(x)$$
            $$P(a < X \leqslant b) = F(b) - F(a)$$

            性质；
            $$0 \leqslant F(x) \leqslant 1, \forall x \in R$$

            $$F(-\infty) = \lim_{x \to -\infty}F(x) = 0$$
            $$F(+\infty) = \lim_{x \to +\infty}F(x) = 1$$
            
            用于解函数中的待定常数

            $$x_1 < x_2 \Rightarrow F(x_1) \leqslant F(x_2)$$
            $$\lim_{x \to x_0^{+}}F(x) = F(x_0), \forall x_0 \in R$$

            随机变量可以分为离散型随机变量或者连续型随机变量。

            定义L若随机变量X的所有值为$x_1, x_2, \cdots x_k$
            $$P(X = x_k) = p_k, k = 1, 2, 3, \cdots$$

            \indent 则称上式为离散型随机变量X的概率分布或者分布律

            $$p_k > 0$$
            $$\sum_{k = 1}^{\infty}p_k = 1$$

            离散型随机变量x的概率分布和分布函数的关系
            $$F(x) = \int_{-\infty}^{x}p(X = x)$$
        \subsection{常用离散型随机变量的分布}
            \subsubsection{0-1分布}
            $$P(X = k) = p^k(1 - p)^{1 - k}, k = 0, 1$$
            \subsubsection{二项分布}
            $$P(X = k) = C_n^kp^k(1 - p)^{n - k}, k = 0, 1 \cdots n$$
            记作:
            $$ X  \sim B(n, p) $$

            二项分布在 $n = 1$ 时退化为0-1分布。 如果$X \sim B(n, p)$则$X = \sum_{i = 1}^nX_i$, 其中它们互相独立， 且$X \sim B(1, p)$
        \subsubsection{泊松分布}
        $$P(X = k) = \frac{\lambda^k}{k!}e^{-\lambda}, k = 0, 1, 2, \cdots (\lambda > 0) $$
            则称X服从参数为$\lambda$的泊松分布：
            $$X \sim P(\lambda)$$

            泊松分布最开始是对于二次分布的一个近似。若对于二项分布中的$n$充分大，$p$充分小时，则$\lambda = np$。
            $$C_n^kp^k(1-p)^{n - k} \approx \frac{\lambda^k}{k!}e^{-\lambda}$$
        \subsubsection{几何分布}
            $$P(X = k) = (1 - p)^{k - 1}p, k = 1, 2, 3, \cdots$$
            
            记作：
            $$X \sim G(p)$$

            一般用于独立重复试验 A首次发生时的概率。
    \subsection{连续型随机变量及其分布}
        定义：设变量$X$的分布函数为$F(x)$, 如哦存在非负可积函数$p(x)$,满足：
        $$F(x) = \int_{-\infty}^{x}p(t)dt$$
        
        则称$X$为连续型随机变量，其中$p(x)$为概率密度函数，简称密度函数。由上面的式子可以看出，分布函数在整个区间上可导(积分上限函数一定可导), 所以分布函数一定可积。概率密度函数的区间面积表示概率。
    
        存在连续型随机变量既不是离散型随机变量，也不是连续型随机变量。

        $$\int_{-\infty}^{\infty}p(t)dt = 1$$
        $$P(x_1 < X \leqslant x_2) = \int_{x_1}^{x_2}p(t)dt$$
        $$P(X = a) = \lim_{h\rightarrow0}\int_{a}^{a + h}p(t)dt = 0$$

        判断一个分布是不是密度函数:
        $$\int_{-\infty}^{\infty}p(t)dt = 1, \quad p(x) \geqslant 0 \ \text{for } \forall x \in R $$
    \subsection{常用连续型随机变量的分布}
        \subsubsection{均匀分布}
        $$
        p(x) = 
        \begin{cases}
            \frac{1}{b - a} & x \in (a, b) \\
            0 & x\notin (a, b)
        \end{cases}
        $$
        $$X \sim U(a, b)$$

        均匀分布中相同长度区间的概率一样，对应于之前的几何概型。  
        
        \subsubsection{指数分布}
        $$
        p(x) = 
        \begin{cases}
            \lambda e^{-\lambda x} & x > 0 \\
            0 & x \leqslant 0 
        \end{cases}
        $$

        $$X \sim EXP(\lambda)$$
    
        $\lambda$ 越大，靠近原点的分布就越密集。拥有无记忆性
        $$\begin{aligned} 
            P(X > s + t | X > s)
            &= \frac{P(X > s  t)}{P(X > s)} \\
            &= \frac{e^{-\lambda(s + t)}}{e ^ {-\lambda s}}\\
            &= P(x > t) 
        \end{aligned}
        $$
        
        无记忆性可以理解为：电子元件的寿命在符合指数分布的情况下，以上式子可以解释为在已经工作了$s$小时的情况下，再工作$t$小时的概率和新的电子原件工作$t$小时的概率一样，和已经正常工作的时间无关。几何概型中也类似满足这恶样的无记忆性。
        
        \subsubsection{正态分布}
        $$p(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x - \mu)^2}{2\sigma^2}},\ x \in R,\ \sigma > 0 $$
        $$X \sim N(\mu, \sigma^2)$$

        标准正态分布:
        $$X\sim N(0,1) \quad \varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$$

        $p(x)$ 关于 $x = \mu$对称, 顶点为 $(\mu, \sqrt{2\pi}\sigma)$; $\mu$ 会导致曲线平移，被陈伟位置参数; $\sigma$ 反映了取值分散性，被称为形状参数。
        
        正态分布于标准正态分布之间的关系, 可以将任意的正态分布转化为标准正态分布
        $$p(x) = \frac{1}{\sigma}\varphi(\frac{x - \mu}{\sigma})$$
        $$F(x) = \Phi(\frac{x - \mu}{\sigma})$$
        $$X \sim N(\mu, \sigma^2),\ Y = \frac{X - \mu}{\sigma} \quad \Rightarrow \quad Y \sim N(0, 1)$$

        证明：

        $$\begin{aligned} 
            F_Y(y)
            &= P(Y \leqslant y) \\
            &= P(\frac{X - \mu}{\sigma} \leqslant y) \\
            &= P(X \leqslant \sigma y + \mu) \\
            &= F(\sigma y + \mu) \\ 
            &= \Phi (y)
        \end{aligned} $$
        
        以上可以推出，$X \sim N(\mu, \sigma^2)$：

        $$\begin{aligned} 
            P(a < X \leqslant b) 
            &= F(b) - F(a) \\ 
            &= \Phi(\frac{b - \mu}{\sigma}) - \Phi(\frac{a - \mu}{\sigma})
        \end{aligned}$$

        标准正态分布的性质:
        $$\Phi(-x) = 1 - \Phi(x)$$
        
        这个时候负值就可由正值得到

        $3\sigma$ 原则
        $$P(|x - \mu| < \sigma)\ \ = 2\Phi(1) - 1 =  0.6826$$
        $$P(|x - \mu| < 2\sigma) = 2\Phi(2) - 1 = 0.9546$$
        $$P(|x - \mu| < 3\sigma) = 2\Phi(3) - 1 = 0.9974$$

        所以我们常常只研究 $(\mu - 3\sigma, \mu + 3\sigma)$ 范围内的概率
    \subsection{随机变量函数的分布}
    $$P(X = x_k) = p_k, \quad Y = f(X)$$
   
    Y一定是一个离散型随机变量，确定y的取值然后的带等价的随机变量分布。
        $$P(Y = y_j) = P(f(X) = y_j, X \in D)$$
    
    计算的时候应该先算更加简单的分布; 连续型随机变量可能会映射为离散型随机变量。连续型随机变量要先算出Y的分布函数，然后再算出概率密度函数。

    当$Y = f(X)$ 可以求出反函数的$X = h(Y)$，则可以先求出密度函数：

    $$p_Y(y) = p_x(h(y)) \cdot |h^{'}(y)|$$
    
    其中$y \in (\alpha, \beta), \quad \alpha = \text{min}(f(-\infty), f(+\infty)), \quad \beta = \text{max}(f(-\infty), f(+\infty))$

    $$X \sim N(\mu, \sigma^2), \quad Y = aX + b \Rightarrow Y \sim N(a\mu + b, a^2\sigma^2)$$

    \subsubsection{习题课}
        书:p62 2; p64 34, 41;
        判别一个函数是不是分布函数：单调不减，$F(-\infty) = 0, F(+\infty) = 0$, 右连续，

        习题集：p52, 6,26,27, 3, 12, 16, 27,28,31, 8, 27;
\newpage

\section{随机向量及其分布}
    \subsection{随机向量}
        \subsubsection{二维随机向量}
            设随机试验$E$的样本空间为$\Omega = \{\omega\}$, 若对于一个$\omega$,都有确定两个单值实$X(\omega), Y(\omega)$函数与之对应，称$(X(\omega), Y(\omega))$ 为二维随机向量，记为 $(X, Y)$

        \subsubsection{联合分布函数}
            对于二维随机向量$(X, Y)$, 则二元函数
            $$F(x, y) = P(X \leqslant x, Y \leqslant y), \quad (x, y) \in R^2$$
        
            为$(X, Y)$的联合分布函数。
            $$P(x_1 < X \leqslant x_2, y_1 < Y \leqslant y_2) = F(x_2, y_2) - F(x_1, y_2) - F(x_2, y_1) + F(x_1, y_1)$$

            逆事件概率:
            $$1 - F(X, Y) = P(X > x) + P(Y > y) - P(X > x, Y > y)$$
            $$F(-\infty, x) = F(y, -\infty) = F(-\infty, -\infty) = 0$$
            $$F(+\infty, +\infty) = 1$$

            $$x_1 < x_2 \Rightarrow F(x_1, y) \leqslant F(x_2, y)$$
            $$y_1 < y_2 \Rightarrow F(x, y_1) \leqslant F(x, y_2)$$

            $$\lim_{\varepsilon \rightarrow 0^+}F(x + \varepsilon, y) = F(x, y)$$
            $$\lim_{\varepsilon \rightarrow 0^+}F(x, y + \varepsilon) = F(x, y)$$
            $$x_1\leqslant x_2, y_1 \leqslant y_2, \quad F(x_2, y_2) - F(x_1, y_2) - F(x_2, y_1) + F(x_1, y_1) \geqslant 0$$
        
        \subsubsection{边际分布}
            对于联合分布函数$F(x, y)$, $F_X(x), F_Y(y)$称为边际分布 
                $$F_X(x) = F(x, +\infty),\quad F_Y(y) = F(+\infty, y)$$
        \subsubsection{二维离散型随机向量}
            $$p_{ij} = P(X = x_i, Y = y_j)$$
            
            称为随机向量的联合分布律
            $$p_{ij} > 0,\quad\sum_{i = 1}^{\infty}\sum_{j = 1}^{\infty}p_{ij} = 1$$
            $$P(X = x_i) = p_{i^{.}} = \sum_{j}^{+\infty}p_{ij}$$
            $$P(y = y_j) = p_{^{.}j} = \sum_{i}^{+\infty}p_{ij}$$
    
    \subsection{二维连续性随机变量常用分布}
        \subsubsection{二维均匀分布}
            设$G$是平面上的一个有界区域，其面积记为$S_G$ 则二维随机向量联合密度函数

            $$p(x,y) = \begin{cases}
                \frac{1}{S_G} & (x, y) \in G \\
                0 & (x, y) \notin G 
            \end{cases}$$
            则服从区域G上的二维均匀分布
            对于矩形区域 $[a, b] \times [c, d]$, 边际分布为 $X \sim U(a, b)$, $Y \sim U(c,d)$

        \subsubsection{二维正态分布}
            $$
                p(x, y) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}e^{-\frac{1}{2(1 - \rho^2)}\lbrack\frac{(x - \mu_1)^2}{\sigma_1^2} - \frac{2\rho(x - \mu_1)(y - \mu_2)}{\sigma_1\sigma_2} + \frac{(y - \mu_2)^2}{\sigma_2^2}\rbrack}
            $$

            记作: $(X, Y) \sim N(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho)$

            边际分布 $X \sim N(\mu_1, \sigma_1^2)$, $Y \sim N(\mu_2, \sigma_2^2)$
        \subsection{随机变量的独立性}
            $$F(x, y) = F_X(x)F_Y(y)$$
            \newpage 

\newpage
\section{随机变量的数字特征}
    \subsection{数学期望}
        对于离散型随机变量，设概率分布为$P(X = x_k) = p_k$, 则数学期望 $EX$ 为
        $$EX = \sum_{k = 1}^{+\infty}x_kp_k$$

        对于连续型随机变量，对于密度函数为$p(x)$, 当
        $$EX = \int_{-\infty}^{+\infty}xp(x)dx$$
      
        绝对收敛时，则数学期望$EX$存在
    
    \subsubsection{随机变量函数的数学期望}
        $$EY = E[f(X)] = \sum_{k = 1}^{+\infty}f(x_k)P(X = k)$$
        $$EY = E(f(x)] = \int_{-\infty}^{+\infty}f(x)p(x)dx $$
        $$EZ = E[f(X, Y)] = \sum_{i = 0}^{+\infty}\sum_{j = 0}^{+\infty}f(x, y)p_{ij}$$
        $$EZ = E[f(x, y)] = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x, y)p(x, y)dxdy$$

    \subsubsection{数学期望的性质}
    $$E(C) = C, \text{ $C$是常数}$$
    $$E(CX) = CE(X)$$
    $$E(\sum_{i = 1}^nX_i) = \sum_{i = 1}^nE(X_i)$$
    
    若两个随机变量相互独立，则
    $$E(XY) = E(X)E(Y)$$

    利用数学期望的可加性，在计算数学期望的时候可以将随机变量拆分成小的随机变量然后计算

    \subsection{方差}
    $$DX = E[(X - EX)^2]$$
    $$\sqrt{DX} = \sqrt{E[(X - EX)^2]} = \sigma(x) \text{ (标准差)}$$
    
    $DX$ 表示了随机变量$X$ 的分散程度，越小表示分散程度越小。$DX$ 也是 $Y = (X - EX)^2$ 的期望

    $$DX = E(X^2) - (EX)^2$$

    \subsubsection{方差的性质}   
    $$D(C) = 0$$
    $$D(CX) = C^2D(X)$$

    当$X_i$相互独立:
    $$D(\sum_{i = 1}^n \pm X_i) = \sum_{i = 1}^nD(X_i)$$
    $$$$

    \subsection{总结}
    $$X \sim B(1, p),\ EX = p,\ DX = p(1 - p)$$
    $$X \sim B(n, p),\ EX = np,\ DX = np(1 - p)$$
    $$X \sim P(\lambda),\ EX = \lambda,\ DX = \lambda$$
    $$X \sim G(P),\ EX = \frac{1}{p},\ DX = \frac{1 - p}{p^2}$$
    
    $$X \sim U(a, b),\ EX = \frac{a + b}{2},\ DX = \frac{(b - a)^2}{12}$$
    $$X \sim Exp(\lambda),\ EX = \frac{1}{\lambda},\ DX = \frac{1}{\lambda^2}$$
    $$X \sim N(\mu, \sigma^2),\ EX = \mu,\ DX = \sigma^2$$



        \newpage
\section{}
        
\end{document}