\documentclass[12pt, a4paper, twocolumn]{article}   
    \usepackage{indentfirst, amsmath, fontspec, listings, xcolor, amssymb}

    \newfontfamily\consolas{Consolas}
    \lstset{numberstyle = \small\consolas, basicstyle=\small\consolas}
        
    \XeTeXlinebreaklocale "zh"
    \XeTeXlinebreakskip = 0pt plus 1pt

\begin{document}
\section*{Normal Distribution}

$$E(X) = \mu, Var(x) = \sigma^2$$
$$\psi_X(t) = E(e^{tx}) = e^{\mu t + \frac{\sigma^2t^2}{2}}$$
$$Y = aX + b \sim N(a\mu + b, a^2\sigma^2)$$
$$\Phi(-x) = 1 - \Phi(x), \Phi^{-1}(P) = -\Phi^{-1}(1 - P)$$
$$X \sim N(\mu, \sigma^2), \frac{x - \mu}{\sigma} \sim N(0, 1)$$
$$\sum_{i = 1}^n \sim N(\sum_{i = 1}^n a_i\mu_i, \sum_{i = 1}^n a_i^2\sigma^2)$$
$$\rm{sample\ mean: \overline{x_n} = \frac{\sum_{i = 1}^nx_i}{n}}$$
$$$$

\section*{Gamma Distribution}
$$\Gamma(x) = \int_0^{\infty}x^{\alpha - 1}e^{-x}dx$$
$$\Gamma(1) = 1, \Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)$$
$$\Gamma(n) = (n - 1)!, \gamma(\frac{1}{2}) = \sqrt{\pi}$$
$$\int_0^{\infty}x^{\alpha - 1}e^{-\beta x}dx = \frac{\Gamma(\alpha)}{\beta^\alpha}$$
$$\rm{pdf}:\ \frac{\beta^\alpha}{\gamma(\alpha)}x^{\alpha - 1}e^{-\beta x}, x > 0$$
$$E(X) = \frac{\beta}{\alpha}, Var(x) = \frac{\alpha}{\beta^2}$$
$$\psi_X(t) = (\frac{\beta}{\beta - t})^{\alpha}$$
$$\sum_{i = 1}^n x_i \sim Gamma(\sum_{i = 1}^n \alpha_i, \beta)$$
$$\rm{pdf\ } Gamma(1, \beta)\ exp(\beta) \ \beta e^{-\beta x}$$
$$x \sim exp(\beta)\ P(x \geqslant t + h | x \geqslant) = p(x \geqslant h)$$

\section*{Law of Large Number}
$$\rm{mankov\ inequity: } P(x \geqslant t) \leqslant \frac{E(x)}{t}$$
$$\rm{chebyshev\ inequity:} P(|x - E(x)| \geqslant t) \leqslant \frac{Var(x)}{t^2}$$

\section*{Central limit theorem}

$$x_1, x_2, .. x_n \ with \ E(X) = \mu, Var(X) = \sigma^2$$
$$\lim_{n\leftarrow \infty}P(\frac{\overline{x_n} - \mu}{\frac{\sigma}{\sqrt{n}}}) = \Phi(x)$$

continuity correction

\section*{Maximum likehood estimator}
find parameter when $f(x_1, x_2, x_3 ... | \alpha)$ reaches the maximum value

\section*{Properties of MLE}
$$\widehat{g(\theta)} = g(\widehat{\theta})$$
M.O.M.E using sample means and variance to estimate parameters 


Geometric distribution 
$$f(x) = p(1 - p)^x$$

\section*{sufficient statistics}
\section*{Improving estimates}
$$R(\theta, \delta) = E((\delta^2 - \theta^2)) $$
$$= Var(\delta) + (E(\delta) - \theta)^2$$
 
$$\delta_0(T) = E(\delta(x)|T) \rm{ min }$$
$$\rm{inadmissible\ = \ not\ a\ function\ of\ T}$$

\section*{chi-square distribution}

gamma($\frac{n}{2}, \frac{1}{2}$)
$$y \sim N(0, 1), y^2 \sim \chi(1)$$
$$\sum x_i \sim \chi(\sum n_i)$$

\section*{Joint distribution of Sample mean and sample variance}
$$x_i \sim N(\mu, \sigma^2)$$
$$\overline{X}_n \sim N(\mu, \frac{\sigma^2}{n})$$
$$\frac{\sum (X_i - \mu)}{\sigma^2} \sim \chi(n)$$
$$\frac{\sum (X_i - \overline{X}_n)^2}{\sigma^2} \sim \chi(n - 1)$$

\section*{T distribution}
$$X_i \sim N(0, 1)$$
$$T(n - 1) = \frac{\overline{X} - \mu}{\frac{S}{\sqrt{n}}},\ S = \frac{1}{n - 1}\sum (X_i - \overline{X})^2$$
$$T(n - 1) = \frac{\overline{X} - \mu}{\frac{S_n}{\sqrt{n - 1}}},\ S = \frac{1}{n}\sum(X_i - \overline{X})^2$$
$$T = \frac{\overline{X} - \mu}{\sqrt{\frac{\sum(X_i - X)^2}{n(n - 1)}}}$$
$$T(m) = \frac{\sqrt{X}}{{\sqrt{\frac{Y}{m}}}},\ X\sim N(0, 1), Y \sim \chi(m)$$
$$pdf\ \frac{\Gamma(\frac{m + 1}{2})}{(m\pi)^{\frac{1}{2}}\Gamma(\frac{m}{2})}(1 + \frac{x^2}{m})^{-\frac{m + 1}{2}}$$

\section*{Confidence interval}

$\mu$ known, $\delta$ unknown 
$$\frac{\overline{X} - \mu}{\frac{\delta}{\sqrt{n}}} \sim N(0, 1)$$
$$\mu \sim [\overline{X} - \frac{\delta}{\sqrt{n}}\Phi^{-1}, \overline{X} + \frac{\delta}{\sqrt{n}}\Phi^{-1}]$$

$\mu$ unknown, $\delta$ known
$$\frac{\sum (X_i - \mu)^2}{\sigma^2} \sim \chi(n)$$
$$\delta^2 \sim [\frac{\sum (X_i - \mu)^2}{\chi_n^{-1}(\alpha)},\frac{\sum (X_i - \mu)^2}{\chi_n^{-1}(\alpha)}]$$

both unknown, $\delta^2$ remains the same calculation
$$\frac{\overline{X} - \mu}{\frac{\sigma^`}{\sqrt{n}}} \sim T(n - 1)$$
$$\mu \in [\overline{X} - \frac{\sigma^`}{\sqrt{n}}T_{n - 1}^{-1}, \overline{X} + \frac{\sigma^`}{\sqrt{n}}T_{n - 1}^{-1}]$$

\section*{Unbiased Estimator}
for estimator $\delta(x)$ and parameter $\theta$, for every $\theta$, $E(\delta) = \theta$: unbiased estimator 
$$MSE = Var(\delta(x)) + \rm{bias} ^ 2$$
$$\rm{bias} = E(\delta(x)) - \theta$$

for mean $\mu$, $\overline{X}$; for $\sigma^2$, $\delta^`$

$$E(\widehat{\sigma}^2) = \frac{n - 1}{n}\sigma^2$$
$$\sigma^{`2} = \frac{n}{n - 1}\widehat{\sigma}^2$$
$$E((\sigma^` - \sigma)^2) = Var(\sigma^{'2})$$
$$ = Var(\frac{\sigma^2}{n - 1}\frac{n - 1}{\sigma^2}\sigma^{2'})$$
$$ = \frac{\sigma^4}{(n - 1)^2}\times 2(n - 1) = \frac{2\sigma^2}{n - 1}$$
$$E((\widehat{\sigma^2} - \sigma^2)) = Var() + \rm{bias}^2$$
$$Var = \frac{\sigma^2}{n^2} \times 2(n - 1)$$
$$\rm{bias} = \frac{\sigma^2}{n^2} $$
$$ = \frac{2(n - 1)\sigma^2}{n^2}$$

\section*{Testing Hypotheses}
$$H_0, H_1$$

simple / composite hypothesis, critical region 

type I / II error, power function $\pi(\delta|\theta)$

size of a test $$\alpha(\delta) = \rm{sup} \pi(\delta|\theta),\ \theta \in \Omega_0$$

$H_0$ is simple, $\alpha(\delta) = \pi(\delta|theta_0)$

\section*{Testing simple Hypotheses}
$H_0: \theta = \theta_0$, $H_1: \theta = \theta_1$
$$\alpha(\delta), \beta(\delta)$$

with smallest $a\alpha + b\beta$ when reject $H_0$ when $af(x|theta_0) < bf(x|theta_1)$ and accept when $af(x|theta_0) > bf(x|theta_1)$

find smallest $\beta$ with known size or $\alpha$, reject when $f(x|\theta_0) < kf(x|\theta_1)$, accept the averse

\section*{UMP test}
MLR: $\frac{f(x_i|\theta_2)}{f(x_i|\theta_1)}$increasing with $T = r(x)$  when $\theta_2 > \theta_1$

For $H_0: \theta \leqslant \theta_0$ and $H_1: \theta > \theta_1$, with test $T > C$, $T$ is the MLR. This is the UMP test. In addition, power function is increasing of $\theta$. If Hypotheses reverse, the test is $T < C$ and decreasing function of $\theta$

\section*{two sided alternatives}

no UMP

usually normal 

using ${\overline{X} < C_1, \overline{X} > C_2}$

if $C_1, C_2$ is  symetrical respect to $\mu_0$, then power function will be symetrical respect to $\mu_0$






\section*{Content}
MSE 

unbiased 

randomized test (b test)

confidence interval $\mu$ $\sigma^2$

t distribution 

\end{document}

